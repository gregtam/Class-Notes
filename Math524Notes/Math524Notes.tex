\documentclass[a4paper,12pt]{amsart}

\usepackage[pdftex]{graphicx}
\usepackage{moreverb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage[all]{xy}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{multirow}
\allowdisplaybreaks

\newcommand{\sumin}{\sum_{i=1}^n}
\newcommand{\sumiN}{\sum_{i=1}^N}
\newcommand{\sumim}{\sum_{i=1}^m}
\newcommand{\sumjn}{\sum_{j=1}^n}
\newcommand{\sumjN}{\sum_{j=1}^N}
\newcommand{\sumjm}{\sum_{j=1}^m}
\newcommand{\prodin}{\prod_{i=1}^n}
\newcommand{\dx}{\,\mathrm{d}x}
\newcommand{\dy}{\,\mathrm{d}y}
\newcommand{\dt}{\,\mathrm{d}t}
\newcommand{\du}{\,\mathrm{d}u}
\newcommand{\dv}{\,\mathrm{d}v}

\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\Corr}{\mbox{Corr}}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\p}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\sP}{\mathbb{P}}

\newtheorem*{prop}{Prop}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{ex}{Example}
\newtheorem{defn}{Definition}

\begin{document}

\title{Math 524 - Nonparametric Statistics}
\author{Greg Tam}
\date{}
\maketitle
\tableofcontents

\section{September 5th, 2012}
\noindent
Classically: $X_1,\ldots,X_n \sim \mathcal{N}(\mu,\sigma^2)$\\
$\mathcal{F}_\theta = \{f_\theta, \theta \in \Theta\}$\\
We will focus more on hypothesis testing in this course. Advanced topics include estimation of $F$ and $f$. We may compare $\widehat{F}$ to $F_{\widehat{\theta}}$ or $\widehat{f}$ to $f_{\widehat{\theta}}$.\\
\\
\noindent
Kolmogorov-Smirnov statistic: $\displaystyle \sup_{x \in \mathbb{R}} \left| F_{\widehat{\theta}}(x) - \widehat{F}(x)\right|$\\
\\
\noindent
Cram\'er von Mises statsitic: $\displaystyle \int_{-\infty}^\infty \! (F_{\widehat{\theta}}(x) - \widehat{F}(x))^2 \dx$

\subsection{Notion of rank}
Given a random sample $X_1,\ldots,X_N$ from an arbitrary distribution $F$, let
\[R_i = \mbox{rank of } X_i\]
\subsection{Working hypothesis}
We generally assume that the data come from a continuous distribution such that there are no ties in rank almost surely. The advantage of using ranks is that it is robust. The assumptions under which the procedures are carried out are minimal. A disadvantage is that there is a loss of information. The mean of the ranks is constant. 

\subsection{Distribution of $(R_1,\ldots,R_N)$}
The distribution of the random vector $(R_1,\ldots R_N)$ is uniformly distributed.
\subsection{Invariance with respect to $F$}
If $F$ is continuous and strictly increasing, then
\begin{align*}
R_i \leq R_j &\Leftrightarrow X_i \leq X_j\\
			&\Leftrightarrow F(X_i) \leq F(X_j)
\end{align*}
\begin{proof}
\[\int_{-\infty}^\infty \! F(x) f(x) \dx  = \int_0^1 u \, \mathrm{d}u = \frac{1}{2}\]
by changing $u = F(x)$, $\mathrm{d}u = f(x)\dx$
\end{proof}

\begin{theorem}
Let $X$ be a random variable with CDF $F$, $F$ continuous. Then the distribution of $F(X)$ is Uniform on $(0,1)$.
\end{theorem}
\begin{proof}
\[F^{-1}(u)  = \inf\{x: F(X) \geq u\}\]
$\forall u, x$, it holds that $F^{-1}(u) \leq x \Leftrightarrow F(x) \geq u$.
\begin{itemize}
\item If $F$ is continuous, then $F(F^{-1}(u)) = u$
\item If $F$ is continuous, then $F(X)$ is continuous, i.e. $\forall x \in \mathbb{R}$, $P(F(X)=u)=0$
\end{itemize}
$P(F(X)=u)=0$ is the same as $P(X = F^{-1}(u))$ or $P(X \mbox{ lies in the flat part of F})$. 
\[P(\underbrace{F(X)}_{\in [0,1]} \leq u) = 
\begin{cases}
0 & u < 0\\
1 & u \geq 1
\end{cases}\]
If $u \in [0,1)$, 
\begin{align*}
P(F(X) \leq u) &= 1 - P(F(X)>u) \\
&= 1 - P(F(X) \geq u) \mbox{ by continuity of } F\\
&= 1-P(X \geq F^{-1}(u)\\
&= P(X \leq F^{-1}(u))\mbox{ by continuity of } F\\
& = F(F^{-1}(u)) = u
\end{align*}
\end{proof}

\begin{remark}
Continuity of $F$ is crucial
\end{remark}
\noindent
$X \sim \mbox{Bernoulli}(p)$\\
$P(X=0) = 1-p$\\
$P(X=1) = p$\\
\[F(X) = \begin{cases}
F(0) = 1-p & \mbox{with prob. } 1-p\\
F(1) = 1 & \mbox{with prob. } p\\
\end{cases}\]


\section{September 10th, 2012}
\[X \prec Y \Leftrightarrow G(y) \leq F(y) \; \forall \; y\] where $X$ has distribution $F$ and $Y$ has distribution $G$. 
\[\sumin i= \frac{n(n+1)}{2}\]

Our hypothesis is of the form
\[H_0: F = G \qquad H_a: F \neq G \mbox{ or } F \prec G\]
but this really means that the distributions have the same location parameter, not that they are exactly the same.
\[W_{XY} = W_S - \frac{n(n+1)}{2}\]
Under $H_0$, any assignment of ranks is equally likely. Using this fact we can tabulate the distribution of $W_{XY}$ and make inference.


\section{September 12th, 2012}
\[ \Var(W_{XY}) = \sum_{(i,j) = (i',j')} \Var(H_{ij}) + \sum_{(i,j) \neq (i',j')} \Cov(H_{ij},H_{i'j'})\]
\begin{align*}
\Var(H_{ij}) &= \frac{1}{2}\left(1 - \frac{1}{2}\right)\\
&= \frac{1}{4}
\end{align*}
\begin{enumerate}
\item[a)] $i \neq i'$, $j \neq j'$
\begin{align*}
\E{H_{ij}H_{i'j'}} &= P(X_i<Y_j,X_{i'}<Y_{j'})\\
&= \frac{1}{4}
\end{align*}
\item[b)] $i = i'$, $j \neq j'$
\begin{align*}
\E{H_{ij}H_{i'j'}} &= P(X_i<Y_j, X_i, Y_{j'})\\
&=P(X_i < \min(Y_j,Y_{j'}))
\end{align*}
\end{enumerate}

\[\E{\overline{Y}_S} = \mu = \frac{1}{N} \sum_{i=1}^N i = \frac{N+1}{2}\]

\[\Var(\overline{Y}_s) = \underbrace{\frac{N-n}{N-1}/n}_\text{finite population factor} \times \frac{1}{N}\sum_{i=1}^N (Y_i-\mu)^2\]

\begin{align*}
\sigma^2 &= \frac{1}{N}\sum_{i=1}^N (Y_i-\mu)^2\\
&= \frac{1}{N}\left\{ \sum_{i=1}^N Y_i^2 - N \mu^2 \right\}\\
&= \frac{1}{N}\left\{\frac{N(N+1)(2N+1)}{6} - N \left(\frac{N+1}{2}\right)^2\right\}\\
&= \frac{N+1}{N} \left\{\frac{N^2-N}{12} \right\}\\
&= \frac{(N-1)(N+1)}{12}\\
\Var(\overline{Y}_s) &= \frac{N-n}{N-1}/n \frac{(N-1)(N+1)}{12}\\
&= \frac{1}{n} \frac{(N-n)(N+1)}{12}
\end{align*}

\section{September 17th, 2012}
\textbf{\textit{Treatment:}} 78, 64, 75, 45, 82 $\sim F$\\
\textbf{\textit{Control:}} 110, 70, 53, 51 $\sim G$\\
We must decide now whether the treatment has any effect without assuming any distribution.\\
\[H_0: F = G \qquad H_1: F \prec G \;( \forall \; x: F(x) \geq G(x))\]

\subsection{Dealing with Ties}
\[\underbrace{17 \; 17 \; 17}_{d_1 = 3} \underbrace{19}_{d_2 = 1} \underbrace{21}_{d_3=1} \]

\subsubsection{General Result on Sampling}
$(y_1,\ldots,y_N) = $ Population\\
Draw a sample without replacement at random.\\
sample: $\{y_i: i \in S\}$ $n = |S| \leq N$\\
$\displaystyle T = \sum_{i \in S} y_i$\\
We wish to compute $\E{T}$ and $\Var(T)$.\\
$\displaystyle \E{T} = \sum_{i \in S} \E{y_i} = n \cdot \overline{y}$ The $y_i$'s are identically distributed (not independent) and 
\[\E{y_i} = \sum_{j=1}^N y_j \frac{1}{N} = \overline{y}\] 
\begin{align*}
\Var(T) &= \sum_{i \in S} \Var(y_i) + \sum_{i,j \in S, i \neq j} \Cov(y_i,y_j)\\
&= n \cdot \underbrace{\Var(y_i)}_{\displaystyle \frac{1}{N} \sum_{j=1}^N (y_j - \overline{y})^2 = \tau^2} + n(n-1) \underbrace{\Cov(y_i,y_j)}_{ = \lambda}\\
\end{align*}
To get the covariance, let $n=N$.
\[\Var(T) =0 = N \tau^2 + N(N-1) \lambda \Rightarrow \lambda = \frac{\tau^2}{N-1} \]
For general $n$, 
\[\Var(T) = n \tau^2 - \frac{n(n-1)\tau^2}{N-1} = n \tau^2\left(1 - \frac{n-1}{N-1}\right) = n \tau^2 \frac{N-n}{N-1}\]
\hrule
\begin{align*}
{\underbrace{W_S^\ast}_{=T}} &= n\left(\frac{d_1R_1 + d_2R_2 + \ldots + d_lR_l}{N}\right)\\
&= \frac{n}{N}\left(\sum_{i=1}^{d_1} i + \sum_{i=1}^{d_2} (d_1 + i) + \ldots \sum_{i=1}^{d_l} (d_1 + \ldots + d_{l-1} + i)\right)\\
&= \frac{n}{N} \frac{N(N+1)}{2}\\
&= \frac{n(N+1)}{2}
\end{align*}
Since
\begin{align*}
R_1 &= \frac{1}{d_1}\sum_{i=1}^{d_1} i = \frac{d_1+1)}{2}\\
R_2 &= \frac{1}{d_2}\sum_{i=1}^{d_2}(d_1 + i) = \frac{d_2 d_1}{d_2} + \frac{d_2(d_2+1)}{2d_2}\\
R_3 &= \frac{1}{d_3} \sum_{i=1}^{d_3} (d_1 + d_2 + i) = d_1 + d_2 + \frac{d_3 +1}{2}\\
&\vdots\\
R_l &= d_1 + d_2 + \cdots + d_{l-1} + \frac{d_l+1}{2}
\end{align*}
Note that for any $a_1 ,\ldots, a_k \in \mathbb{R}$, $\overline{a} = \frac{1}{k}\sum a_i$, 
\[\sum_{i=1}^k (a_i - \overline{a})^2 = \sum_{i-1}^k a_i^2 - k \overline{a}^2 \Leftrightarrow k \overline{a}^2 = \sum_{i-1}^k a_i^2 - \sum_{i=1}^k (a_i - \overline{a})^2\]
Now set $a_i =i$, $\overline{a} = (k+1)/2$
\begin{align*}
{i=1}^k \left(i - \frac{k+1}{2}\right)^2 &= \sum_{i=1}^k i^2 - k \left(\frac{k+1}{2}\right)^2\\
&= \frac{k(k+1)(2k+1)}{6} - {i=1}^k \left(i - \frac{k+1}{2}\right)^2\\
&= \frac{k(k^2-1)}{12}\\
&= \frac{k^3-k}{12}
\end{align*}
Hence
\[k \overline{a}^2 - \sum a_i^2 - \frac{k^3-k}{12}\]
\begin{align*}
\sum_{i=1}^{d_1} R_i^2 &= d_1R_1^2 = d_1\left(\frac{d_1+1}{2}\right)^2 = \sum_{i=1}^{d_1}i^2 - \frac{d_1^3-d}{12}\\
\sum_{i=d_1+1}^{d_2}R_i^2 &= d_2R_2^2 = d_2\underbrace{\left(d_1 + \frac{d_2 + 1}{2}\right)^2}_{\displaystyle \overline{a} = \frac{1}{d_2}\sum_{i=1}^{d_2} (d_1 + i)}\\
&= \sum_{i=1}^{d_2}(d_1+i)^2 - \frac{d_2^3 - d_2}{12}\\
\sum_{i=1}^{d_l} R_l^2 &= d_l\underbrace{R_l^2}_{\mathclap{\displaystyle \frac{1}{d_l} = \sum_{i=1}^{d_l}(d_1 + \ldots + c_{l-1} + i)}} = \sum{i=1}^{d_l} (d_1 + \ldots + d_{l-1} + i)^2 - \frac{d_l^3-d_l}{12}
\end{align*}
\begin{align*}
\frac{1}{N} \sum_{i=1}^N \left(R_i^\ast - \frac{N+1}{2}\right)^2 &= \frac{1}{N}\sum (R_i^\ast)^2 - \left(\frac{N+1}{2}\right)^2\\
&= \frac{1}{N}\left(\sum_{i=1}^{d_1} i^2 - \frac{d_1^3 - d_1}{12} + \sum_{i=1}^{d_2} (d_1+i)^2 - \frac{d_2^3-d_2}{12} +\right. \\
&\ldots \left.+ \sum_{i=1}^{d_l} (d_1 + \ldots + d_{l-1} + i)^2 - \frac{d_l^3-d_l}{12}\right)\\
&=\frac{1}{N}\left(\sum_{i=1}^N i^2 \right) - \frac{1}{N}\sum_{j=1}^l \frac{d_j^3-d_j}{12} - \frac{(N+1)^2}{4}\\
&= \frac{N^2-1}{12} - \frac{1}{N}\sum_{j=1}^l \frac{d_j^3-d_j}{12}\\
&= \tau^2 
\end{align*}
\begin{align*}
\Var(W_S^\ast) &= n\frac{(N-n)(N^2-1)}{(N-1)12} - \frac{n(N-n)}{(N-1)N}\sum \frac{d_j^3-d_j}{12}\\
&= \frac{nm(N+1)}{12} - \frac{nm}{N(N-1)12} \sum \frac{d_j^3-d_j}{12}
\end{align*}
$\max_i d_i/N$ cannot be too close to 1 or the variance is equal to 0.

\[H_0: F =G \qquad H_1: F \succ G\]
\begin{align*}
P(W_S^* \geq 1720 ) &= 1 - P(W_S^* < 1720)\\
&= 1 - P(W_S^* \leq 1719)
\end{align*}


\section{September 19th, 2012}
\begin{defn}[Omnibus Test]
\[\begin{cases}
H_0: F=G\\
H_1: F \neq G
\end{cases}\]
\end{defn}
\noindent
\textbf{Idea:}\\
Estimate $F$ by $F_m$ and $G$ by $G_n$. We check whether the ``distance'' between $F_m$ and $G_n$ is large.
\[\|F_m - G_n\| = 
\begin{cases}
\sup_t |F_m(t) - G_n(t)|\\
\int (F_m(t)-G_n(t))^2 \dt
\end{cases}
\]

\begin{align*}
F_m(x) &= \frac{1}{m}\sumim \mathds{1}(X_i\leq x)\\
Z_i &= \mathds{1}(X_i \leq x) \sim \mbox{Bernoulli}(F(x))\\
P(Z_i = 1) &= P(X_i \leq x) = F(x)\\
\end{align*}
so
\begin{align*}
F_m(x) &= \frac{1}{m}\sumim Z_i\\
&\to F(x) \mbox{ almost surely by SLLN}\\
&= \E{Z}
\end{align*}
\noindent
Stronger result(Glivenko-Cantelli Theorem):\\
\[\sup_{x \in \mathbb{R}} |F_m(x) - F(x)| \to^{a.s.} 0 \]

\begin{itemize}
\item $H_1: F \neq G$
\[D_{m,n} = \sup_{t \in \mathbb{R}} |F_m(t) - G_n(t)|\]
\item $H_1: H \prec G$ $(F(t) \geq G(t))$
\[D_{m,n}^+ = \sup_{t \in \mathbb{R}} (F_m(t) - G_n(t))\]
\item $H_1: H \succ G$
\[D_{m,n}^- = \sup_{t \in \mathbb{R}} (G_n(t) - F_m(t))\]
\end{itemize}


\noindent
Given $(X_1,\ldots,X_m)$ and $(Y_1,\ldots,Y_n)$, we look a the pooled sample, $(X_1,\ldots,X_m,Y_1,\ldots,Y_n) = (X_1^*, \ldots, X_{n+m}^*)$ and rank them
$(R_1^*, \ldots, R_{m+n}^*) = (R_1,\ldots,R_m,S_1,\ldots,S_n)$

\begin{align*}
&\sup_x F_m(x) - G_n(x)\\
 &= \max_{1 \leq i \leq n+m} F_m(X_i^*) - G_n(X_i^*)\\
&= \max_{1 \leq i \leq n+m} \left(\frac{1}{m}\sum_{j=1}^m \mathds{1}(X_j \leq X_i^*) - \frac{1}{n}\sum_{j=1}^n \mathds{1}(Y_j \leq X_i^*)\right)\\
&= \frac{n+m}{nm}\max_{1 \leq i \leq n+m} \left(\frac{n}{n+m}\sum_{j=1}^m \mathds{1}(X_j \leq X_i^*) - \frac{m}{n+m}\sum_{j=1}^n \mathds{1}(Y_j \leq X_i^*)\right)\\
&= \frac{n+m}{nm}\max_{1 \leq i \leq n+m} \left(\frac{n}{n+m}\sum_{j=1}^m \mathds{1}(X_j \leq X_i^*) - \frac{m}{n+m}\sum_{j=1}^n \mathds{1}(Y_j \leq X_i^*) + \right.\\
&\left.\frac{n}{n+m}\sum_{j=1}^m \mathds{1}(Y_j \leq X_i^*)-\frac{n}{n+m}\sum_{j=1}^m \mathds{1}(Y_j \leq X_i^*)\right)\\
&=\frac{n+m}{nm}\max_{1 \leq i \leq n+m} \left(\frac{n}{n+m} \sum_{j=1}^{n+m} \mathds{1}(X_j^* \leq X_i^*) - \sum_{j=1}^n \mathds{1}(Y_j \leq X_i^*)\left[\frac{m}{n+m} + \frac{n}{m+n}\right]\right)\\
&= \frac{n+m}{nm}\max_{1 \leq k \leq n+m} \left(\frac{n}{n+m}k - \sum_{j=1}^n \mathds{1}(S_j \leq k) \right)
\end{align*}

For the asymptotic result given by Gnedenko and Koklyuk, we require that \[\frac{n}{n+m} \to \eta \in (0,1)\] since we require it to be a two sample problem.


\section{September 24th, 2012}
\[G(x) = \p{X + \Delta \leq x} = \p{X \leq x - \Delta} = F(x - \Delta)\]

We have
\[G(x) - F(x-\Delta) \leq F(x)\]
since $F$ is increasing and $G$ is stochastically bigger than $F$.
\\

The power, $\pi(\Delta)$, is equal to 
\[\mathbb{P}_{F,G}\left(\mbox{Test rejects } H_0\right)\]
or equivalently in our case, 
\[\mathbb{P}_{F,\Delta}\left(\mbox{Test rejects } H_0\right)\]

\[\pi(0) = \alpha \approx \mathbb{P}_{F,\Delta}(W_s > c)\]
Since the Wilcoxon test is discrete, we cannot find a $c$ such that the power is exactly $\alpha$. If $m$ and $n$ are small, then $\pi(0) < \alpha$. If $\min(m,n) \to \infty$, we can approximate the statistic by the normal distribution and $\pi(0) \approx \alpha$.

We hope that $\pi(\Delta)$ is as close to 1 as possible. \\
Is it true that $0 < \Delta_1 < \Delta_2 \Rightarrow \pi(\Delta_1) \leq \pi(\Delta_2)$?
\begin{proof}
\[Y_1, \ldots, Y_n \sim F(x - \Delta_1)\]
\[Y_1 - \Delta_1, \ldots, Y_n - \Delta_1 \sim F\]
\[Y_1 + \Delta_2 - \Delta_1, \ldots, Y_n + \Delta_2 - \Delta_1 \sim F(x - \Delta_2)\]
\[\p{Y_i + \Delta_2 - \Delta_1 \leq x} = \p{Y_i - \Delta_1 \leq x - \Delta_2} = F(x- \Delta_2)\]
We let $V_i = Y_i + \Delta_2 - \Delta_1$.
\\
\\
\[W_{XY} = W_S - \frac{n(n+1)}{2} =\sumim \sumjn \mathds{1}(X_i < Y_j)\]
\[W_{XV} = \sumim \sumjn \underbrace{\mathds{1}(X_i < V_j)}_{\mathclap{ \mathds{1}(X_i < Y_j + \underbrace{\Delta_2 - \Delta_1}_{>0})}}\]
\[W_{XY} \leq W_{XV} \Rightarrow W_{XY} > c + \frac{n(n+1)}{2} \Rightarrow W_{XV} > c + \frac{n(n+1)}{2}\]

\noindent
Our treatment group is:\\
$Y_1, \ldots, Y_n \sim F(x- \Delta_1)$\\
$V_1^*, \ldots, V_n^* \sim F(x - \Delta_2)$\\
However, it is not necessarily true that $V_i^* = Y_i + \Delta_2 - \Delta_1$. Hence 
\begin{align*}
\{W_{XY} >c \} \subset \{W_{XV} > c\} &\Rightarrow \p{W_{XY} >c} \leq \p{W_{XV} > c}\\
&\Rightarrow \pi(\Delta_1) \leq \pi(\Delta_2)
\end{align*}
\end{proof}

From symmetry of the normal distribution, we have
\[1 - \Phi(z) = \Phi(-z)\]
Then
\begin{align*}
\pi(F,G) &= \Phi\left( \frac{\E{W_{XY}} - c}{\sqrt{\Var(W_{XY})}}\right)\\
&= \Phi\left(\frac{mnp_1 - z_\alpha \sqrt{\frac{mn(N+1)}{2}} - \frac{mn}{2}}{\sqrt{\Var(W_{XY})}}\right)\\
p_1 &= \p{X<Y} \qquad \left(= \frac{1}{2} \mbox{ under} H_0\right)\\
&= \int_{-\infty}^\infty \! F(y) g(y) \dy\\
F^\ast(t) &= \p{X - X' \leq t}, \; X \perp X',\; X\sim X' \sim F\\
&= \int_{-\infty}^\infty F(t + x') f(x') \dx'\\
f^\ast(t) &= \int_{-\infty}^\infty f(t + x') f(x') \dx'\\
p_1 &= \p{X< X' + \Delta}\\
&= \p{X - X' < \Delta}\\
&= F^*(\Delta)\\
F^\ast(\Delta) &\approx F^*(0) + \frac{f^*(0)}{1!}\Delta + \ldots\\
p_1 &\approx \frac{1}{2} + \frac{f^*(0)}{1!}\Delta
\end{align*}

Now we have
\[\frac{mn(\frac{1}{2} + f^*(0)\Delta - \frac{1}{2}) - z_\alpha \sqrt{\frac{mn(N+1)}{12}}}{\sqrt{\frac{mn(N+1)}{12}}} = \sqrt{\frac{12}{N+1}}f^*(0) \Delta \sqrt{mn} - z_\alpha\]

\section{September 26th, 2012}
\[f^*(0) = \int_{-\infty}^\infty \! f(t)^2 \dt\]
\[F \sim \mathcal{N}(0, \sigma^2) \qquad G \sim \mathcal{N}(\Delta,\sigma^2)\]
$f^*$ is the density of $X-X'$, $X',X \sim F$, and $X' \perp X$. We have $X-X' \sim \mathcal{N}(0,2\sigma^2)$.
\[f^*(0) = \frac{1}{2 \sqrt{\pi}\sigma}\]
\[\pi(\Delta) \approx \Phi\left(\sqrt{\frac{12mn}{m+n+1}}\Delta f^*(0) - z_\alpha \right)\]
\[\Phi(z_\alpha) = 1 - \alpha \]

\begin{ex}
$\Delta \approx 2$, $\sigma^2 \approx 2$
\[\pi(\Delta) = 1 - \beta\]
To make it easier, we assume $m=n$, which implies
\[\sqrt{\frac{12mn}{n+m+1}} = \sqrt{\frac{12n^2}{2n+1}} \approx \sqrt{6n} \]
\begin{align*}
\pi(\Delta) &= 1 - \beta = \Phi(z_\beta)\\
&\approx \Phi\left(\sqrt{6n} \frac{\Delta}{2 \sqrt{\pi} \sigma} - z_\alpha\right)
\end{align*}
\[\Leftrightarrow \sqrt{6n} \frac{1}{2 \sqrt{\pi}}\frac{\Delta}{\sigma} - z_\alpha = z_\beta\]
so we have
\[n = \frac{(z_\beta + z_\alpha)^2 4 \pi \sigma^2}{6 \Delta^2}\]
\end{ex}

For a large number of repetitions ($N=1000$) do:
\begin{itemize}
\item[Step 1.] Sample $X_1, \ldots, X_m$ from $\mathcal{N}(0,\sigma^2)$ and sample $Y_1,\ldots, Y_n$ from $\mathcal{N}(\Delta,\sigma^2)$ independently.
\item[Step 2.] Wilcoxon Test $(X,Y)$. return the $p$-value. Store all $p$-values.
\item[End.] We will end up with $N=1000$ $p$-values.
\end{itemize}
\noindent
How often did the test reject? We look at the number of $p$-values which are less than $\alpha$. The probability of the rejection is thus
\[\frac{\mbox{rej}}{N} \approx \p{\mbox{test rejecting }H_0 \mbox{ under our scenario}}\]
\begin{align*}
\p{\mbox{rejection}} &= \p{W_S>c}\\
&= 12 \p{X_1<X_2<Y_1<Y_2<Y_3}
\end{align*}
\[X \sim F \qquad Y \sim G \qquad \Rightarrow \frac{X}{a} \sim G\]

\noindent
$\log X$, $\log Y = \log X - \log a$
\begin{align*}
\p{\log X \leq x} &= \p{X \leq e^x}\\
&= 1 - \exp(-e^x) \mbox{ Gumbel distribution}
\end{align*}
Here, $\Delta = - \log a$.\\
\\
\\
Suppose that $\Var(F) < \infty$ ($H_0: F=G$)\\
To compute the critical level, we need the distribution of the $t$-statistic under $H_0$.
\[\frac{\overline{Y} - \overline{X}}{S \sqrt{\frac{1}{m} + \frac{1}{n}}}\]
Central Limit Theorem:\\
\[\sqrt{n}\frac{\overline{Y} - \E{Y}}{\sqrt{\Var(F)}} \overset{n \to \infty}{\leadsto} \mathcal{N}(0,1)\]
\[\sqrt{m}\frac{\overline{X} - \E{X}}{\sqrt{\Var(F)}} \overset{n \to \infty}{\leadsto} \mathcal{N}(0,1)\]
\[\frac{S}{\sigma} \to^P 1\]
\begin{align*}
\frac{\overline{Y} - \overline{X}}{S} \sqrt{\frac{mn}{m+n}} &= \frac{\sigma}{S} \frac{\overline{Y} - \overline{X} \pm \mu}{\sigma} \sqrt{\frac{mn}{m+n}}\\
&= \frac{\sigma}{S} \left( \frac{(\overline{Y}- \mu)\sqrt{n} \sqrt{\frac{m}{n+m}}}{\sigma} - \frac{(\overline{X}- \mu) \sqrt{m} \sqrt{\frac{n}{n+m}}}{\sigma}\right)\\
\end{align*}
\noindent
The $\frac{\sigma}{S}$ term converges to 1 and the rest converges to 
\[\mathcal{N}(0,1)\lambda + \mathcal{N}(0,1) (1-\lambda) \sim \mathcal{N}(0,1)\]
where $\lambda = \sqrt{\frac{m}{n+m}}$ as $(m,n) \to \infty$ and $\frac{m}{n+m} \to \lambda \in (0,1)$.



\section{October 1st, 2012}
\noindent
\textbf{Wilcoxon test:} for $\Delta$ small,
\[\mbox{Power} = \pi(\Delta) \approx \Phi \bigg(\sqrt{\frac{12mn}{m+n}} \underbrace{f^*(0)}_{\mathclap{\displaystyle \int_{-\infty}^\infty \! f(x)^2 \dx}} \Delta - z_\alpha \bigg)\]
\textbf{T-test:}
\begin{align*}
\pi(\Delta) &\approx \Phi\left(\Delta \frac{1}{\sigma \sqrt{1/m + 1/n} - z_\alpha} \right) \\
&= \Phi\left(\sqrt{\frac{mn}{m+n}}\frac{1}{\sigma}\Delta - z_\alpha \right)
\end{align*}

\subsection{Comparing Wilcoxon to $t$-test}
How do we compare the Wilcoxon and $t$-test for the shift model when under $H_0$, the hypothesized distribution is $F$.\\
\textbf{Idea 1:} look at the power curve $\pi$ in a neighbourhood of $\Delta=0$.
\[\Delta_n = \frac{h}{\sqrt{n}}\]
\textbf{Wilcoxon: (Test 1)}
\[\pi(\Delta_n) = \Phi\left(\sqrt{6} f^*(0)h - z_\alpha\right)\]
\textbf{T-test: (Test 2)}
\[\pi(\Delta_n) = \Phi\left(\frac{1}{\sqrt{2} \sigma}h - z_\alpha\right)\]
These curves are asymptotic powers. Test 1 is better than test 2 if the slope of the asymptotic power curve at 0 is bigger for test 1 than for test 2.\\
\textbf{Test 1:}
\[\Phi'(-z_\alpha)\sqrt{6}f^*(0)\]
\textbf{Test 2:}
\[\Phi'(-z_\alpha) \frac{1}{\sqrt{2}\sigma}\]
So this is true if 
\[\Phi'(-z_\alpha)\sqrt{6}f^*(0)> \Phi'(-z_\alpha) \frac{1}{\sqrt{2}\sigma}\]
or equivalently if
\[\sqrt{12}f^*(0) \sigma > 1\]
\\
\\
\noindent
\textbf{Idea 2:}\\
\textbf{Test 1:} Compute the sample size to reach power $\beta$ at level $\alpha$.
\[n_1 = \frac{(z_\alpha + z_\beta)^2}{\Delta^2 6 \{f^*(0)\}^2}\]
\textbf{Test 2:}
\[n_2 = \frac{(z_\alpha + z_\beta)^2 \sigma^2 2}{\Delta^2}\]
We prefer the test that has a smaller sample size so we look at the relative efficiency. 
\begin{align*}
\frac{n_2}{n_1} &= \frac{\frac{(z_\alpha + z_\beta)^2 \sigma^2 2}{\Delta^2}}{\frac{(z_\alpha + z_\beta)^2}{\Delta^2 6 \{f^*(0)\}^2}}\\
&= 12\sigma^2 \{f^*(0)\}^2
\end{align*}
Test 1 is better than test 2 if $12 \sigma^2 \{f^*(0)\}^2>1$. The ratio
\[\frac{n_2}{n_1} \underset{n \to \infty}{\longrightarrow} \mbox{ is called Pitman's asymptotic relative efficiency}\]


\subsection{Optimal Tests}
\[W_S = \sumin S_i\]
\[\underbrace{X_1, \ldots, X_m}_{\text{controls}}, \underbrace{Y_1, \ldots, Y_n}_{\text{treatments}}\]
We have $R_1^*, \ldots R_N^*$ which are the ranks of the pooled sample.
\[W_S = \sum_{i=m+1}^N R_i^* = \sum_{i=1}^N C_{N,i} a_{N,R_i^*}\]
The $C_{N,i}$ are called coefficients and the typical choice of these coefficients in the two-sample problem is:
\[C_{N,i} =\begin{cases}
0 & \mbox{ if } i \in \{1,\ldots,m\}\\
1 & \mbox{ if } i \in \{m+1,\ldots,N\}
\end{cases} \]
\textbf{Wilcoxon test: } $a_{N,R_i^*} = R_i^*$\\
\textbf{Question: } How can we compute the scores as to achieve maximal power?\\
\\
\noindent
\textbf{General Result:} Assume that $X_1, \ldots, X_n$ have density $f$ and $Y_1, \ldots, Y_n$ have density $f_\theta$. Then under smoothness conditions on $f$, the optimal score function 
\[a_{N,k} = \mathbb{E}_{\theta_0}\left(-\frac{\left.\frac{\partial f}{\partial \theta}\right|_{\theta_0}}{f_{\theta_0}} \circ F_{\theta_0}^{-1}(\mathcal{U}_{(k)})\right)\]
where
\[\mathcal{U}_1, \ldots \mathcal{U}_n \mbox{ are from } \mathcal{U}(0,1)\]
and
\[\mathcal{U}_{(1)} \leq \mathcal{U}_{(2)} \leq \ldots \leq \mathcal{U}_{(N)}\]
If $X_1, \ldots, X_n$ are from $F_{\theta_0}$, then
\[X_{(1)} \leq \ldots \leq X_{(N)} \overset{d}{=} F_{\theta_0}^{-1}(\mathcal{U}_{(1)}) \leq \ldots \leq F_{\theta_0}^{-1}(\mathcal{U}_{(N)})\]


\begin{ex}
$f_{\theta_0}$ is the density of $\mathcal{N}(0,1)$. $f_\theta$ is the density of $\mathcal{N}(\Delta,1)$.
\[f_\theta = \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{(x-\Delta)^2}{2}\right\}\]
\[\left.\frac{\partial f_\Delta}{\partial \Delta}\right|_{\Delta=0} = \frac{1}{\sqrt{2 \pi}} e^{-x^2/2}(-x)\]
\[\frac{\left.\frac{\partial f_\Delta}{\partial \Delta}\right|_{\Delta=0}}{f_0} = x\]
So our optimal scores are thus
\[\E{\Phi^{-1}(\mathcal{U}_{(k)})} = \E{X_{(k)}}\]
\end{ex}
Suppose that the exact scores are
\[A_{N,k} = \E{\phi(\mathcal{U}_{(k)})}\]
Then if $\phi$ is well behaved, then the test using the approximative scores: 
\[A_{N,k}^* = \phi(\E{\mathcal{U}_{(k)}}) = \phi\left(\frac{k}{N+1}\right)\]
achieves the same asymptotic power (i.e. the same efficiency)

\begin{ex}
Suppose we had the shift model
\[X_1,\ldots, X_m \text{ with density } f(x)\]
\[Y_1, \ldots, Y_n \text{ with density } f(x_\Delta)\]
\begin{align*}
\frac{\partial f}{\partial \Delta} &= - f'(x-\Delta)\\
\left.\frac{\partial f}{\partial \Delta}\right|_{\Delta=0} &= - f'(x)\\
\end{align*}
Optimal exact scores: $a_{N,i} = \E{-\frac{f'}{f}(F^{-1}(\mathcal{U}_{(k)})}$\\
Approximative scores: $a_{N,i} = -\frac{f'}{f}(F^{-1}\left(\frac{k}{N+1}\right))$\\
Now let $F(X) = \frac{1}{1+e^{-x}}$, $x \in \mathbb{R}$ (logistic distribution)
\[f(x) = \frac{e^{-x}}{(1+e^{-x})^2}, \; x \in \mathbb{R}\]
Optimal scores in the logistic shift model 
\begin{align*}
f'(x) &= \frac{(-e^{-x})(1 + e^{-x} - 2 e^{-x})}{(1 + e^{-x})^3}\\
&= \frac{(-e^{-x})(1-e^{-x})}{(1+e^{-x})^3}\\
F^{-1}(u) &= \ln \left(\frac{u}{1-u}\right)\\
-\frac{f'(x)}{f(x)} &= \frac{e^{-x}(1-e^{-x})(1+e^{-x})^2}{(1+e^{-x})^3e^{-x}}\\
&= \frac{1-e^{-x}}{1+e^{-x}}\\
-\frac{f'}{f}(F^{-1}(u)) &= \frac{1 - \frac{1-u}{u}}{1 + \frac{1-u}{u}}\\
&= \frac{u-1 + u}{u+1 - u}\\
&= 2u-1
\end{align*}
exact: $\E{2 \mathcal{U}_{(k)}-1} = 2 \frac{k}{N+1}-1$
so we have that our optimal test
\[\sumin 2 \frac{S_i}{N+1} -1 \varpropto W_S\]
\end{ex}

\begin{ex}
$X_1, \ldots, X_m$ have density $f$ and $Y_1, \ldots, Y_n$ have density $\theta f(1-F)^\theta$\\
$1 - (1-F)^\theta$ are called the Lehmann alternatives
\[\frac{f_\theta}{1 - F_\theta} = \theta \frac{f}{1-F}\]
\[\phi(u) = 1 + \ln (1-u)\]
(in the book there is a minus sign instead of a plus but Johanna said she did the calculation multiple times..) In the exact case this is equal to
\[\mathbb{E}\big[- \underbrace{\ln (1-\mathcal{U}_{(k)})}_{X_{(k)}}\big]\]
and in the approximate case we have
\[- \ln\left(1 - \frac{k}{N+1}\right)\]
\end{ex}


\section{October 3rd, 2012}
\subsection{Lehmann Alternatives}
\[H_1: G = 1 - (1-F)^\Delta\]
then
\[g = \Delta(1-F)^{\Delta-1}f\]
Hazard rate: $\displaystyle \frac{\p{X=t}}{\p{X>t}} = \frac{f(t)}{1-F(t)} = \lambda(t)$
\[\frac{\Delta (1-F(t))^{\Delta-1}f(t)}{(1-F(t))^\Delta} = \Delta \frac{f(t)}{1-F(t)}\]

\begin{align*}
H_0: &\; \lambda = \mu\\
H_1: &\; \mu = \Delta \cdot \lambda, \; \Delta >0
\end{align*}

\subsection{Estimation of Treatment Effect}
Assume $F$ is continuous.
\begin{align*}
H_0: &\; F = G\\
H_1: &\; G = F(x- \Delta)
\end{align*}
Suppose we had $X \sim F$, $Y \sim G$. Then
\[Y \overset{d}{=} X + \Delta\]
We have that $Y \overset{d}{=}X + \Delta$. Then
\[\E{Y-X} = \E{Y} - \E{X} = \Delta\]
or also
\[Y-X \overset{d}{=} X' + \Delta - X = (X' - X) + \Delta\]
where $X'-X$ is symmetric around 0, this is all symmetric around $\Delta$. We also have
\[\text{med}(Y-X) = \Delta\]

\begin{ex}
Suppose we have $X_1, \ldots X_n \sim \mathcal{N}(\mu,1)$
\[\overline{X} \sim \mathcal{N}\left(0,\frac{1}{n}\right) \qquad \overline{X} \sim \mathcal{N}-\mu \left(0,\frac{1}{n}\right)\]
and there is no dependency on $\mu$.
\end{ex}

\noindent
$(X_1,\ldots,X_m) = X$\\
$(Y_1,\ldots,Y_n) = Y$\\
\[\widehat{\Delta}(X,Y) = \text{med}(Y_j - X_i) , \; i \in \{1,\ldots, m\}, j \in \{1, \ldots, n\}\]
so 
\[\widehat{\Delta}(aX +b,aY+b) = a \widehat{\Delta}(X,Y)\]
\\
\begin{align*}
\widehat{\Delta} \text{ is symmetric around } \Delta &\Leftrightarrow \widehat{\Delta} - \Delta \text{ is symmetric around } 0\\
&\Leftrightarrow \widehat{\Delta} \text{ is symmetric around 0 in the case } F=G
\end{align*}


\begin{ex}
Given $Y_1, X_1, X_2$, we have our possible differences are $Y_1-X_1$ and $Y_1-X_2$. 
\begin{align*}
\widehat{\Delta} &= \frac{Y_1 - X_1 + Y_1 - X_2}{2}\\
&= Y_1 + \frac{X_1 + X_2}{2} \overset{d}{\neq} \frac{X_1 + X_2}{2} - Y_1
\end{align*}
\[Y_1 - X_1 \overset{d}{=} X_1 - Y_1 \]
\end{ex}

If $F$ is continuous, it can be shown that $\widehat{\Delta}$ is also continuous, i.e. $\p{\widehat{\Delta}=x}=0$ $\forall \; x \in \mathbb{R}$. If $mn$ is odd, then 
\[\p{\widehat{\Delta} < \Delta} = \p{\widehat{\Delta} > \Delta} = \frac{1}{2}\]

In the case where $mn$ is even,
\[D_{(k)} < \widehat{\Delta} = \frac{D_{(k)} + D_{(k+1)}}{2}< D_{(k+1)}\]
\[\p{\widehat{\Delta} \leq 0} \leq \p{D_{(k)}\leq 0}\]
\[\p{\widehat{\Delta} \leq 0} \geq \p{D_{(k+1)}\leq 0}\]


\section{October 10th, 2012}
\subsection{Consistency of $\widehat{\Delta}$}
\[\widehat{\Delta}_{mn} \overset{\mathbb{P}}{\to}\Delta \]
\[\forall \; \varepsilon > 0,\; \p{|\widehat{\Delta}-\Delta|> \varepsilon} \to 0 \text{ as } mn \to \infty \Leftrightarrow \p{|\widehat{\Delta} - \Delta| \leq \varepsilon} \to 1\]

If $mn$ is odd, then it is equal to $2k+1$, $\widehat{\Delta} = D_{(k)}$
\begin{align*}
\mathbb{P}_\Delta(|\widehat{\Delta} - \Delta| \leq a)&= \mathbb{P}_\Delta(\Delta - a \leq \widehat{\Delta} \leq \Delta + a)\\
&= \mathbb{P}_\Delta(\widehat{\Delta} - \Delta \leq a) - \mathbb{P}_\Delta(\widehat{\Delta} - \Delta \geq -a)\\
&= \mathbb{P}_0(\widehat{\Delta}\leq a) - \mathbb{P}_0(\widehat{\Delta}\geq -a)\\
&= \mathbb{P}_0(D_{(k)} \leq a) - 1 + \mathbb{P}_0(D_{(k)} \leq -a)\\
&= \mathbb{P}_0(W_{X,Y-a} \leq k+1) - 1 + \mathbb{P}_0(W_{X,Y+a}\leq k+1)\\
&= \mathbb{P}_0\left(\frac{W_{X,Y-a} - p_1mn}{\sqrt{\Var(W_{X,Y-a})}} \leq \frac{\frac{mn}{2} + \frac{1}{2} - p_1mn}{\sqrt{\Var(W_{X,Y-a})}}\right) - 1 \\
&\quad + \mathbb{P}_0\left(\frac{W_{X,Y+a} - p_1^*mn}{\sqrt{\Var(W_{X,Y+a})}} \leq \frac{\frac{mn}{2} + \frac{1}{2} - p_1^*mn}{\sqrt{\Var(W_{X,Y+a})}}\right)\\
&= \Phi\left(\frac{mn(\frac{1}{2}-p_1)}{\sqrt{\Var(W_{X,Y-a})}}\right) -1  + \Phi\left(\frac{mn(\frac{1}{2}-p_1)}{\sqrt{\Var(W_{X,Y+a})}}\right) 
\end{align*}
where
\begin{align*}
p_1 &= \p{X< Y-a}\\
p_1^* &= \p{X < Y + a}
\end{align*}

\subsection{Paired $Z$-test versus Classical $Z$-test}
We observe that the power of the paired test is bigger than the power of the classical test iff $\varrho>0$.

Suppose we have 
\[\begin{array}{cccc}
& i=1 & i=2 & i=3 \\
\hline
\text{C/T}& 5.2 & 4.8 & 4.1\\
\text{T/C}& 5.4 & 4.2 & 4.0
\end{array} \]
where we do not know which one corresponds to the control and which one corresponds to the treatment. We have that 
\begin{align*}
H_0: & \text{ Treatment and control allocation }\\
& \p{\text{1st is control}} = \p{\text{2nd is control}} = \frac{1}{2}
\end{align*}

Our $S_N$ is binomially distributed.
\[\sqrt{N}\frac{\frac{1}{N}S_n - \frac{1}{2}}{\sqrt{\frac{1}{4}}} \approx \mathcal{N}(0,1)\]
\[\frac{2}{\sqrt{N}}S_n - \sqrt{N} \approx Z\]
so
\[S_N \approx Z \frac{\sqrt{N}}{2} + \frac{N}{2} \approx \mathcal{N}\left(\frac{N}{2}, \frac{N}{4}\right)\]



\section{October 15th, 2012}
Suppose we have
\[\begin{array}{c|c|c|c|c}
\hline
\text{Controls} & X_i & 5.2 & 4.7 & 5.8\\
\hline
\text{Treatment} & Y_i & 5.1 & 5.0 & 5.2\\
\hline
& Y_i - X_i & -0.1 & 0.3 & -0.6\\
\hline
\end{array} \]

\[S_n = 2 \qquad \mathbb{P}(\underbrace{S_N}_{\mathclap{\displaystyle \text{Bin}\left(3,\frac{1}{2}\right)}} \geq 2)\]


\[\begin{array}{c|c|c|c}
\hline
|Y_i - X_i| & 0.1 & 0.3 & 0.6\\
\hline
\text{Ranks of } |Y_i - X_i| & -1 & 2 & -3\\
\hline
\end{array}\]
\[V_r = 1 + 3 = 4 \qquad \p{V_r \geq 4}\]

The random part of this is the sign ($+$ or $-$). To calculate the $p$-value, we use this fact. Our possibilities are
\[\begin{array}{c|c|c}
& \text{Pr} & V_S\\
--- & 1/8 & 0\\
--+ & 1/8 & 3\\
-+- & 1/8 & 2\\
+-- & 1/8 & 1\\
-++ & 1/8 & 5\\
+-+ & 1/8 & 4\\
++- & 1/8 & 3\\
+++ & 1/8 & 6
\end{array} \]
So
\[V_S  = \begin{cases}
0 & \text{with prob. } \frac{1}{8}\\
1 & \text{with prob. } \frac{1}{8}\\
2 & \text{with prob. } \frac{1}{8}\\
3 & \text{with prob. } \frac{2}{8}\\
4 & \text{with prob. } \frac{1}{8}\\
5 & \text{with prob. } \frac{1}{8}\\
6 & \text{with prob. } \frac{1}{8}\\
\end{cases} \]
and we have
\[\p{V_S \leq 2} = \p{V_S \in \{0,1,2\}} = \frac{3}{8}\]

\subsection{Moments of $V_S^*$}
Denote the mid-ranks of $|D_1|,\ldots,|D_n|$ as $a_1, \ldots, a_N$.
\begin{align*}
\E{V_S^*} &= \E{\sum_{\{i: X_i \neq Y_i\}} a_i I_i} \qquad I_i \sim \text{Ber}(1/2)\\
&= \sum_{\{i: X_i \neq Y_i\}} a_i \frac{1}{2}\\
&= \sumiN a_i \frac{1}{2} - \frac{1}{2}\left(\frac{d_0 + 1}{2}\right)d_0\\
&= \frac{1}{2}\frac{N(N+1)}{2} - \frac{1}{2}\left(\frac{d_0 + 1}{2}\right)d_0
\end{align*}
We must note that we have $\p{I_i=1} \neq \p{Y_i > X_i}$ but actually is equal to $\p{Y_i > X_i | X_i \neq Y_i}$

\begin{align*}
\Var(V_S^*) &= \sum_{\substack{i=1\\X_i \neq Y_i}}^N a_i^2 \frac{1}{4}\\
&= \frac{1}{4}\underbrace{\sumiN a_i^2}_{\mathclap{\displaystyle \frac{N(N+1)(2N+1)}{6} - \sum_{j=0}^l \frac{d_j(d_j^2-1)}{12}}} - \frac{(d_0+1)^2}{4}d_0\frac{1}{4}\\
&= \frac{N(N+1)(2N+1)}{24}\\
&\quad - \frac{d_0(d_0+1)(2d_0+1)}{24}\\
&\quad - \frac{\sum_{i=1}^l d_i(d_i+1)(d_i-1)}{48}
\end{align*}

\[V_S^* = 17.5\]
\[\E{V_S^*} = \frac{7 \cdot 8}{4} - \frac{3 \cdot 4}{4} = 14 - 3 = 11\]
\[\Var(V_S^*) = \frac{7 \cdot 8 \cdot 15}{24} - \frac{3 \cdot 4 \cdot 7}{24} - \frac{2 \cdot 3 + 2\cdot 3}{48} = 31.25\]
We usually do not use the continuity correction for approximation of $p$-values since the values of the statistics are not necessarily integers.



\section{October 22nd, 2012}
\subsection{Multiple Data Blocks}
Suppose we would like to compare whether live lectures or online lectures. Let's say the grades are tabulate as follows:

\begin{center}
\begin{tabular}{lccccc}
\hline
Live ($G$) & 72 & 75 & 83 & 95 & 100\\
Online ($F$) & 68 & 69 & 74 & 82 & 93\\
\hline
\end{tabular}
\end{center}
We could test $H_0: G = F$ versus $H_1: G \succ F$, in which case we could have
\[W_{S_1} = 3 + 5 + 7 + 9 + 10 = 34\]
where
\[\p{W_S \geq 34} = 0.11\]
Suppose that the data were from year 1 and we look at the data again for year 2. 
\begin{center}
\begin{tabular}{lccccc}
\hline
Live ($G_2$) & 54 & 59 & 60 & 70\\
Online ($F_2$) & 47 & 51 & 52 & 56\\
\hline
\end{tabular}
\end{center}
However, the distribution of $F$ may not be the same as $F_2$.
Now if we rank the data for year 2, we get
\[W_{S_2} = 4 + 6 + 7 + 8 = 25\]

We take 
\[W_S^{combo} = \sum_{k=1}^b c_k W_{S_k}\]
where 
\[c_k = \frac{1}{N_k+1}\]
This is our optimal choice. So we have
\[W_S^{combo} = \frac{34}{11} + \frac{25}{9} = 5.869\]

\begin{displaymath}
\xymatrix{
& W_{S_1}\ar[rd] & \\
\frac{1}{{N_1 \choose n_1}{N_2 \choose n_2}}\ar[ur]\ar[dr] & & W_S^{combo} = \frac{W_{S_1}}{11} + \frac{W_{S_2}}{9}\\
& W_{S_2}\ar[ru] & 
}
\end{displaymath}

\begin{align*}
\E{W_S^{combo}} &= \frac{5+4}{2} = 4.5\\
\Var(W_S^{combo}) &= \frac{5 \cdot 5}{12 \cdot 11} + \frac{4 \cdot 4}{12 \cdot 9} = 0.3375
\end{align*}
Using this knowledge, we approximate
\begin{align*}
\p{W_S^{combo} \geq 5.869} &= \p{\frac{W_S^{combo}-4.5}{\sqrt{0.3375}} \geq \frac{5.869 - 4.5}{\sqrt{0.3375}}}\\
&\approx 1 - \Phi(2.3558)\\
&= 0.00924
\end{align*}

\subsection{Power of the Sign Test}
Suppose we have
\[(X_1,Y_1), \ldots, (X_N,Y_N) \quad  iid\]
and we want to test $H_0$ that there is no treatment effect against $H_1$ that the treatment increases the response.
However here, $X_i$ and $Y_i$ are not necessarily independent.
We would like to see that under the null, $\p{Y-X >0} = 1/2$ and under the alternative $\p{Y-X>0}>1/2$.

Looking at  $\p{Y-X >0} = 1/2$ is not good enough however. We may also consider if the distribution of $Y-X$ is symmetric around 0 ($Y-X \overset{d}{=} X-Y$). But even this isn't good enough!

\noindent\\
\textbf{Caveat for exchangeability:}
\[\begin{pmatrix}
X\\Y
\end{pmatrix} \overset{d}{=} \begin{pmatrix}
Y\\X
\end{pmatrix}
\Rightarrow X \overset{d}{=} Y \; (F=G)\]
but the converse is not true.

Under the shift model, if $Y_i - \Delta \overset{d}{=} X_i$, then 
\[\begin{pmatrix}
X\\Y-\Delta
\end{pmatrix}\]
is exchangeable.

Exchangeability implies $Y-X$ is symmetric around 0 which implies $P(Y-X>0)=1/2$. However, the converses are certainly not true. But even in reality, we are actually using the fact that
\[\p{Y_i-X_i>0} = \frac{1}{2} \quad \forall \; i\]

\section{October 24th, 2012}
\subsection{Sign Test}
Denote 
\[Z = Y - X\]
Under the null, we assume $Z \sim L$ where $L$ is a cdf.\\
\textbf{Sign Test:} 
\begin{itemize}
\item $(X_i,Y_i)$ for $i=1,\ldots,n$ are iid.\\
$H_0$: $(X,Y) \overset{d}{=} (Y,X)$\\
$H_1: (X,Y-\Delta), \; \Delta >0$ is exchangeable.\\
This is the most restrictive hypothesis.
\item $Z_1, \ldots, Z_n$ iid and\\
$H_0: L$ is symmetric about $0$\\
$H_1:$ L is slanted towards positive values.\\
or also $H_1^*: Y-\Delta-X$ is symmetric about 0 with $\Delta >0$.
\item $Z_1, \ldots, Z_n$ are \underline{independent}.\\
$H_0: \p{Z_i>0} = \frac{1}{2} \; \forall i = 1, \ldots, N$ \\
$H_1: \p{Z_i >0} = p > \frac{1}{2} \; \forall i = 1, \ldots, N$.
\end{itemize}

We have our statistic
\[S_N = \sumiN \mathds{1}(Z_i > 0) \sim \mbox{Bin}(N,p)\]
and so 
\[\frac{S_N - Np}{\sqrt{Np(1-p)}} \sim \mathcal{N}(0,1)\]
Hence we use 
\begin{align*}
\mathbb{P}_0(S_N>c) &= \p{\frac{S_N-\frac{N}{2}}{\sqrt{\frac{N}{4}}} > \frac{c-\frac{N}{2}}{\sqrt{\frac{N}{4}}}}\\
&\approx 1 - \Phi\left(\frac{c-\frac{N}{2}}{\sqrt{\frac{N}{4}}}\right)\\
&\approx \alpha
\end{align*}
and we choose $c$ such that
\[\frac{c-\frac{N}{2}}{\sqrt{\frac{N}{4}}} = z_\alpha\]

Now if we want our power to be $\pi_0$, that is we want
\[\pi(p) = \pi_0\]
then we must choose
\[\frac{N(\frac{1}{2}-p) + z_\alpha \sqrt{\frac{N}{4}}}{\sqrt{Np(1-p)}} = z_{\pi_0} = -z_{1-\pi_0}\]
$$DIAGRAMS$$
And hence we solve $N$ such that
\[\frac{\sqrt{N}(\frac{1}{2} -p) + \frac{z_\alpha}{2}}{\sqrt{p(1-p)}} \leq z_{\pi_0}\]
which results in 
\begin{align*}\sqrt{N} &\geq \frac{\frac{z_\alpha}{2} - z_{\pi_0}\sqrt{p(1-p)}}{p-\frac{1}{2}}\\
&\geq  \frac{\frac{z_\alpha}{2} + z_{1-\pi_0}\sqrt{p(1-p)}}{p-\frac{1}{2}}\\
N & \geq \frac{\left(\frac{z_\alpha}{2} + z_{1 - \pi_0}\sqrt{p(1-p)}\right)^2}{(p-\frac{1}{2})^2}
\end{align*}

\subsection{Sign Wilcoxon Test}
Suppose that $L$ is symmetric about $0$. Then
\[
\underbrace{\mathds{1}(Z>0)}_{\text{Bernoulli}(1/2)}
\]
is independent of $|Z|$. To prove this we would like to show that 
\[\p{\mathds{1}(Z>0)=1, \; |Z| \leq z} = \frac{1}{2}\p{|Z| \leq z}\]
and
\[\p{\mathds{1}(Z>0)=0, \; |Z| \leq z} = \frac{1}{2}\p{|Z| \leq z}\] $\forall z \in \mathbb{R}$.
\begin{proof}
\begin{align*}
\p{Z<0,|Z|\leq z} &= \p{0 < Z < z}\\
&=\frac{1}{2}\p{|Z| \leq z} \qquad \text{by symmetry} 
\end{align*}
\end{proof}
We have that
\begin{align*}
(\mathds{1}(Z_1>0), \ldots, \mathds{1}(Z_N>0)) &\indep (|Z_1|,\ldots,|Z_N|)\\
&\indep \text{the ranks of }|Z_1|, \ldots, |Z_N|
\end{align*}
so the probability that we have any combination of $+$ and $-$ signs given the ranks is $\left(\frac{1}{2}\right)^N$.
\[\p{(+,-,+,\ldots,-) | S_1 = s_1, \ldots, S_N = s_n} = \left(\frac{1}{2}\right)^N\]

\subsection{Power of Wilcoxon's Signed-rank Test}
We could like to compute
\[\pi(\Delta) = \sP_\Delta(V_S \geq c)\]
where
\[Y_i - \Delta - X_i \sim L\]

The Mann-Whitney statistic was given as 
\[W_{XY} = \sumim \sumjn \underbrace{\mathds{1}(Y_j>X_i)}_{\mathds{1}(Y_j-X_i>0)}\]
so similarly, we have
\[V_S = \sum_{j=1}^N \sum_{i=1}^j \underbrace{\mathds{1}(Z_j + Z_i >0)}_{\mathds{1}(Y_j - X_j + Y_i - X_i > 0)}\]
We note that
\[\sum_{i=1}^j \mathds{1}(Z_j + Z_i>0) =\begin{cases}
0 & \text{if } Z_j < 0\\
j & \text{if } Z_j > 0 
\end{cases} \]
If we want to compute the expectation, then we have
\[
\E{V_S} = \sum_{j=1}^N \sum_{i=1}^j \E{\mathds{1}(Z_i + Z_j > 0)}\]
\begin{itemize}
\item if $i \neq j$
\[\E{\mathds{1}(Z_i + Z_j > 0 )} = \p{Z + Z' >0} =q\]
\item if $i=j$
\[\E{\mathds{1}(Z_i + Z_i> 0 )} = \p{Z_i > 0 } = p\]
\end{itemize}
\begin{align*}
\E{V_S} &= N p + { N \choose 2} q\\
&= Np + \frac{N(N-1)}{2}q
\end{align*}

To compute the variance, we write it as
\[V_S = \underbrace{\sum_{i < j} \mathds{1}(Z_i + Z_j > 0)}_{V_S^*} + \underbrace{\sum_{i=1}^N \mathds{1}(Z_i > 0)}_{S_N}\]
and then we have
\[\Var(V_S) = \Var(V_S^*) + \underbrace{\Var(S_N)}_{Np(1-p)} + 2\Cov(V_S^*,S_N) \]
Since
\[
\Var(V_S^*) = \underbrace{\sum_{i < j} \Var(\mathds{1}(Z_i + Z_j > 0))}_{\frac{N(N-1)}{2}q(1-q)} + \underbrace{\sum_{i < j} \sum_{k < l} \Cov(\mathds{1}(Z_i + Z_j > 0 ),\mathds{1}(Z_k + Z_l > 0))}_{A}\]
Now we need to look at $A$. We have to consider if any of the indices are different. If they are all different, then the covariance is 0. If there are three of them, then we have either $i=k$, $i=l$, $j=k$, or $j=l$. Hence
\[\p{Z_k + Z_j > 0, Z_k + Z_l < 0 } - \p{Z_k + Z_j > 0}\p{ Z_k + Z_l < 0 }  = r-q^2\]
So we have
\begin{align*}
A &= (r-q)^2 \left\{{N \choose 2}{N \choose 2} - {N \choose 2} - {N \choose 2}{N-2 \choose 2}\right\}\\
&= (r-q^2)N(N-1)(N-2)
\end{align*}


\section{October 29th, 2012}
Looking at last class notes, we had
\[p = \p{Z>0}\]
where $Z = Y-X$ which is not necessarily symmetric about zero.
\[q = \p{Z + Z' > 0}\]
where $Z \indep Z'$ and $Z \overset{d}{=}Z'$.
\[r = \p{Z + Z' > 0, Z + Z''>0}\]
where $Z \overset{d}{=} Z' \overset{d}{=}Z''$ and $Z \indep Z' \indep Z''$.

Now if $H_0$ holds, i.e. $Y-X$ has a continuous distribution which is symmetric about zero. We have
\[p = \frac{1}{2}\]
since $Z$ is symmetric. 
\[q = \frac{1}{2}\]
also since $Z \overset{d}{=} -Z$ meaning $Z + Z' \overset{d}{=} -Z - Z'$, so $Z + Z'$ is also symmetric about zero.
\[r = \frac{1}{3}\]
We get this by doing the following calculation:
\begin{align*}
&\quad\int_{-\infty}^\infty \! \p{Z + Z' >0, Z+ Z''>0|Z=z} \,\mathrm{d}L(z)\\
&=\int_{-\infty}^\infty \! \p{Z' >-z, Z''>-z} \,\mathrm{d}L(z)\\
&=\int_{-\infty}^\infty \! \p{-Z'\leq z, -Z''\leq -z} \,\mathrm{d}L(z)\\
&= \int_{-\infty}^\infty \! \{L(z)\}^2 \, \mathrm{d}L(z)\\
&= \E{\{L(z)\}^2}\\
&= \E{\mathcal{U}^2}\\
&= \int_0^1 \! u^2 \,\mathrm{d}u\\
&= \frac{1}{3}
\end{align*}
Then we have that under $H_0$, 
\begin{align*}
\E{V_S} &= \frac{1}{4}N(N-1) + \frac{N}{2} = \frac{N(N+1)}{4}\\
\Var(V_S) &= N(N-1)(N-2)\frac{1}{12} + \frac{N}{4} + \frac{1}{2}N(N-1)\frac{3}{4}\\
&= \frac{N(2N^2 - 6N + 4 + 9N - 9 + 6)}{24}\\
&= \frac{N(2N^2 + 3N + 1)}{24}\\
&= \frac{N(2N+1)(N+1)}{24}
\end{align*}

\noindent
\textbf{Recall: }
\[\sP\bigg(\underbrace{\frac{V_S - \mathbb{E}_0(V_S)}{\sqrt{\Var_0(V_S)}}}_{\sim \mathcal{N}(0,1)} > \frac{c - \mathbb{E}_0(V_S)}{\sqrt{\Var_0(V_S)}}\bigg) \approx 1 - \Phi\left(\frac{c-\mathbb{E}_0(V_S)}{\sqrt{\Var_0(V_S)}}\right)\]
so then 
\[\frac{c - \mathbb{E}_0(V_S)}{\sqrt{\Var_0(V_S)}} \approx z_\alpha\]
so we choose $c$ such that
\[c = z_\alpha\sqrt{\Var_0(V_S)} + \mathbb{E}_0(V_S)\]

So if we have $H_1$: $Y - \Delta - X$ is symmetric about zero (and continuous) then
\[\pi(\Delta) = \mathbb{P}_\Delta (V_S>c)\]
and we want 
\[\pi(0) \approx \alpha\]
Again, we should expect this function to be increasing with respect to $\Delta$.

We estimate this curve with the normal 

\begin{align*}
\mathbb{P}\bigg(\underbrace{\frac{V_S - \mathbb{E}_\Delta(V_S)}{\sqrt{\Var_\Delta(V_S)}}}_{\sim \mathcal{N}(0,1)}> \frac{c - \mathbb{E}_\Delta(V_S)}{\sqrt{\Var_\Delta(V_S)}}\bigg) &\approx 1 - \Phi\left(\frac{c-\mathbb{E}_\Delta(V_S)}{\sqrt{\Var_\Delta(V_S)}}\right)\\
&= \Phi\left(\frac{\mathbb{E}_\Delta(V_S) - z_\alpha \sqrt{\Var_0(V_S)} - \mathbb{E}_0(V_S)}{\sqrt{\Var_\Delta(V_S)}}\right)
\end{align*}
If we have $(X,Y) \sim \mathcal{N}_2\left(\begin{pmatrix}
\mu_1\\ \mu_2
\end{pmatrix}, \Sigma\right)$, then
\[Y-X \sim \mathcal{N}(\underbrace{\Delta}_{\mathclap{=\mu_2 - \mu_1}}, \overbrace{\tau^2}^{\mathclap{=\sigma_{11} + \sigma_{22} - 2\sigma_{12}}})\]
Under $H_0$: $Y-X \sim \mathcal{N}(0,\tau^2)$\\
Shift alternative $H_1$: $Y - X \sim \mathcal{N}(\Delta, \tau^2)$, $\Delta >0$.
We have that
\begin{align*}
p &= \mathbb{P}_\Delta (Z>0)\\
&= \mathbb{P}_\Delta \bigg(\underbrace{\frac{Z - \Delta}{\tau}}_{\sim \mathcal{N}(0,1)} > - \frac{\Delta}{\tau}\bigg)\\
&= 1 - \Phi\left(\-\frac{\Delta}{\tau}\right)\\
&= \Phi\left(\frac{\Delta}{\tau}\right)
\end{align*}
We also have
\[Z  + Z' \sim \mathcal{N}(2\Delta, 2 \tau^2)\]
so
\begin{align*}
q &= \p{Z + Z' > 0}\\
&= \Phi\left(\frac{2 \Delta}{\sqrt{2} \tau}\right)\\
&= \Phi\left(\sqrt{2} \frac{\Delta}{\tau}\right)
\end{align*}
Now if $S = Z + Z'$ and $T = Z + Z''$, then
\begin{align*}
\Cov(S,T) &= \Cov(Z + Z' , Z + Z'')\\
&= \Var(Z) + \Cov(Z,Z'') + \Cov(Z',Z) + \Cov(Z',Z'')\\
&= \Var(Z)
\end{align*}
since all the other covariances are zero by independence.
\[\begin{pmatrix}
Z + Z'\\ Z+ Z''
\end{pmatrix} =\underbrace{\begin{pmatrix}
1 & 1 & 0 \\ 1 & 0 & 1
\end{pmatrix}}_{A} \underbrace{\begin{pmatrix}
Z \\ Z' \\ Z''
\end{pmatrix}}_{\mathclap{\displaystyle \sim \mathcal{N}_3\left(
\underbrace{
\begin{pmatrix}
\Delta\\ \Delta \\ \Delta
\end{pmatrix}}_{\mu}
,
\underbrace{
\begin{pmatrix}
\tau^2 & 0 & 0 \\
0 & \tau^2 & 0 \\
0 & 0 & \tau^2
\end{pmatrix}}_{\Sigma}
 \right)}} \]
 and we have 
\[A\cdot \mu =
\begin{pmatrix}
1 & 1 & 0 \\ 1 & 0 & 1
\end{pmatrix}\begin{pmatrix}
\Delta \\ \Delta \\ \Delta
\end{pmatrix} = \begin{pmatrix}
2 \Delta \\ 2 \Delta
\end{pmatrix} \]
and
\begin{align*}
r &= \p{Z + Z' >0, Z + Z'' >0}\\
&= \p{S>0, T>0}\\
&=?
\end{align*}

\hrule \bigskip
Let us consider the power function. If $\Delta \approx 0$, then $\Var_\Delta(V_S) \approx \Var_0(V_S)$. So then
\[\pi(\Delta) \approx \Phi\left(\frac{\mathbb{E}_\Delta(V_S) - \mathbb{E}_0(V_S)}{\sqrt{\Var_0(V_S)}} - z_\alpha\right)\]
We use the fact that
\[\mathbb{E}_\Delta(V_S) = \frac{N(N-1)}{2}q + Np\]
so
\[\mathbb{E}_\Delta(V_S) - \mathbb{E}_0(V_S) = \frac{N(N-1)}{2}\left\{q- \frac{1}{2}\right\} + N \left\{p - \frac{1}{2}\right\}\]
We know that 
\[q = \sP_\Delta(Z + Z'>0)\]
Let us denote the CDF of $Z + Z'$ by $L^*$. Then we have
\begin{align*}
q &= \sP_\Delta(Z+ Z' > 0)\\
&= \sP_\Delta(Z - \Delta + Z' - \Delta > - 2\Delta)\\
&= 1 - L^*(-2\Delta)\\
&= L^*(2 \Delta)
\end{align*}
where the last line is true as $L^*$ is symmetric about zero. Now we note that
\begin{align*}
q - \frac{1}{2} &= L^*(2 \Delta) - L^*(0) \\
&\approx l^*(0) 2 \Delta
\end{align*}
where $l^*(0) = (L^*)'(0)$.

Similarly, we have
\begin{align*}
p - \frac{1}{2} &= \sP_\Delta(Z >0) - \frac{1}{2}\\
&= \sP_\Delta(Z - \Delta > - \Delta) - \frac{1}{2}\\
&= 1 - L(-\Delta) - \frac{1}{2}\\
&= L(\Delta) - L(0)\\
&\approx l(0) \Delta
\end{align*}
Now when we plug these in, we get
\begin{align*}
\mathbb{E}_\Delta(V_S) - \mathbb{E}_0(V_S) &= \frac{N(N-1)}{2}\left\{q- \frac{1}{2}\right\} + N \left\{p - \frac{1}{2}\right\}\\
& \approx \Delta \left\{\frac{N(N-1)}{2}2l^*(0) + Nl(0)\right\}\\
&= \Delta \left\{N(N-1)l^*(0) + Nl(0)\right\}
\end{align*}
\hrule \bigskip
Now suppose we want to choose an $N$ such that $\pi(\Delta) = 1- \beta$.
We wish to choose $N$ such that
\[\frac{N(N-1)l^*(0) + Nl(0)}{\sqrt{\displaystyle \frac{N(N+1)(2N+1)}{24}}}\Delta - z_\alpha \geq z_\beta\]
This can be approximated to roughly
\[\frac{N^2 l^*(0) + N l(0)}{\sqrt{\frac{2N^3}{24}}} \geq \frac{z_\beta + z_\alpha}{\Delta} \Leftrightarrow \sqrt{12}\sqrt{N}l^*(0) + \frac{l(0)}{\sqrt{N}} \geq \frac{z_\beta + z_\alpha}{\Delta}\]
For $N$ large, the $\frac{l(0)}{\sqrt{N}}$ is not significant, so we ignore it.
So we choose $N$ such that
\[\sqrt{12}\sqrt{N}l^*(0) \geq \frac{z_\beta + z_\alpha}{\Delta}\]
which is the same as
\[N \geq \frac{(z_\alpha + z_\beta)^2}{\Delta^2 12 \{l^*(0)\}^2}\]
This is almost like the previous case, but instead of $f^*(0)$ being the derivative of $F^*$ where $F^*$ is the cdf of $X - X'$, we have $l^*(0)$ as the derivative of $L^*$, which is the cdf of $Z + Z' \overset{d}{=} Z - Z'$.

\bigskip \hrule \bigskip

\[Z + Z' \sim \mathcal{N}(0,2 \tau^2)\]

so
\begin{align*}
l^*(t) &= \frac{1}{\sqrt{2\pi}}\frac{1}{\tau \sqrt{2}}e^{-t^2/2}\\
l^*(0) &= \frac{1}{\sqrt{2\pi}\sqrt{2} \tau}
\end{align*}

If we want to compute the asymptotic power, we use the fact that
\[\frac{\sqrt{N} \bar{Z}}{S} \sim \mathcal{N}(0,1)\]
and under $H_0$, $Z \sim \mathcal{N}(\Delta,\tau^2)$.\\
Paired $t$-test: $\pi(\Delta) = \Phi\left(\frac{\Delta \sqrt{N}}{\tau} - z_\alpha\right)$

\noindent
\textbf{Note that}
\[p - \frac{1}{2} = L(\Delta) - L(0) \approx l(0) \Delta\]
For the sign test, for $\Delta \approx 0$,
\[\pi(\Delta) \approx 1 - \Phi\{z_\alpha - 2\sqrt{N}\Delta l(0)\}\]


\section{October 31st, 2012}
\subsection{ARE of Wilcoxon Sign Test versus Paired $\boldsymbol t$-test}
\quad\\
\textbf{$\boldsymbol t$-test: }
\[N_t \geq \frac{(z_\alpha + z_\beta)^2 \tau^2}{\Delta^2}\]\\
\textbf{Wilcoxon test:} 
\[N_{V_S} \geq \frac{(z_\alpha + z_\beta)^2}{\Delta^2 \cdot 12 \{l^*(0)\}^2}\]
where $l^*(0)$ is the density of $L^*$, with CDF $Z + Z'$. 
\[e_{V_S,t} = \frac{\frac{(z_\alpha + z_\beta)^2 \tau^2}{\Delta^2}}{\frac{(z_\alpha + z_\beta)^2}{\Delta^2 \cdot 12 \{l^*(0)\}^2}} = 12 \tau^2 \{l^*(0)\}^2\]
If $L \sim \mathcal{N}(0,\tau^2)$, then we know that $L^*$ is a CDF of mean $0$ and variance $2\tau^2$.
\[l^*(x) = \frac{1}{\sqrt{2\pi}} \frac{1}{\tau \sqrt{2}} e^{-\frac{x^2}{4\tau^2}}\]
\hrule \bigskip 	
In the case when $L \sim \mathcal{N}(0,\tau^2)$
\[e_{V_S,t} = 12 \tau^2 \frac{1}{\tau^2 4 \pi} = \frac{3}{\pi}\]

\subsection{Comparison between the three tests}
\qquad\\
\textbf{Sign test:}
\[\pi(\Delta) = \Phi\left(\frac{\sqrt{N}(p - \frac{1}{2}) - z_{\alpha/2}}{\sqrt{p(1-p)}}\right)\]
where
\begin{align*}
p &= \p{Z>0}\\
&= \p{Z-\Delta >  - \Delta}\\
&= 1 - L(- \Delta)\\
&= L(\Delta)
\end{align*}
When $\Delta \approx 0 $, we have 
\[\sqrt{p(1-p)} \approx \frac{1}{2}\]
 and
\[p- \frac{1}{2} = L(\Delta) - L(0) \approx \Delta \cdot l(0)\]
This means that
\[\pi(\Delta) \approx \Phi\left(\frac{\sqrt{N}l(0)\Delta}{\frac{1}{2} - z_\alpha}\right)\]
and so we want
\[2 \sqrt{N_s} l(0) \Delta  - z_\alpha \geq z_\beta\]
which means we want a sample size of
\[N_s \geq \frac{(z_\alpha + z_\beta)^2}{4 \Delta^2 \{l(0)\}^2}\]

\subsection{ARE of Sign Test versus Wilcoxon Test}
\[E_{S,V_S} = \frac{\frac{(z_\alpha + z_\beta)^2}{\Delta^2 12 \{l^*(0)\}^2}}{\frac{(z_\alpha + z_\beta)^2}{\Delta^2 4 \{l(0)\}^2}} =  \frac{\{l(0)\}^2}{\{3l^*(0)\}^2}\]
Note also that
\[l^*(0) = \int_{-\infty}^\infty \! \{l(t)\}^2 \dt\]
Now if we have $L \sim \mathcal{N}(0,\tau^2)$, then
\begin{align*}
l^*(0) &= \frac{1}{2\tau \sqrt{\pi}}\\
l(0) &= \frac{1}{\tau \sqrt{2 \pi}}
\end{align*}
which gives us 
\[e_{S,t} = \frac{2}{\pi} \qquad e_{S,V_S} = \frac{2}{3}\]

\subsection{Estimation of the Treatment Effect}
We have that our efficiencies between the three tests are given by
\begin{align*}
e_{V_S,t} &= 12 \tau^2 \{l^*(0)\}^2\\
e_{S,t} &= 4 \tau^2 \{l(0)\}^2\\
e_{S,V_S} &= \frac{\{l(0)\}^2}{3\{l^*(0)\}^2}
\end{align*}
Now if $Z = Y-X \sim L_\Delta$ and
\begin{align*}
H_0: &\; L \text{ symmetric about }0\\
H_1: &\; L_\Delta = L( \bullet - \Delta) \text{ symmetric about } 0
\end{align*}
Now suppose that $\E{Z} < \infty$. Then under $H_1$, $\E{Z} = \Delta$, $\mbox{med}(Z) = \Delta$, and $\mbox{med} \left(\frac{Z + Z'}{2}\right) = \Delta$. 

If \[\Delta^*(Z_1, \ldots Z_n)\]
is symmetric about $\Delta$ if and only if
\[\Delta^*(Z_1, \ldots, Z_n) - \Delta \]
is symmetric about 0. 
\[\Delta^*(\underbrace{Z_1 - \Delta}_{\sim L}, \ldots, \underbrace{Z_n - \Delta}_{\sim L}) \overset{d}{=} \Delta^*(\Delta - Z_1, \ldots, \Delta - Z_n) =  - \Delta^*(Z_1 - \Delta, \ldots, Z_n - \Delta)\]
since we have that $\forall i$, $Z_i - \Delta \overset{d}{=} \Delta - Z_i$ as $L$ is symmetric about 0.

When will be choose $\bar{\Delta}$ over $\widehat{\Delta}$?
We have this when 
\[\tau^2 \leq \frac{1}{12 \{l^*(0)\}^2}\]
which occurs exactly when 
\[12 \tau^2 \{l^*(0)\}^2 \leq 1\]
which is when the $t$-test is preferable to the Wilcoxon test!


Similarly, $\bar{\Delta}$ is better than $\tilde{\Delta}$ if
\[\tau^2 \leq \frac{1}{4 \{l(0)\}^2} \Leftrightarrow e_{S,t} \leq 1\]
which occurs when the $t$-test is better than the Sign test.

\bigskip \hrule \bigskip

\[\bar{\Delta} = \frac{1}{N}\sumiN Z_i\]
so we have
\[\sqrt{N} \frac{\bar{\Delta} - \Delta}{\tau} \leadsto \mathcal{N}(0,1)\]
by the Central Limit Theorem so 
\[\sqrt{N}(\bar{\Delta} - \Delta) \leadsto \mathcal{N}(0,\tau^2)\]

For 
\[\tilde{\Delta}: Z_{(1)} \leq \ldots \leq Z_{(N)}\]
(assume $N$ is odd for simplicity)
$\tilde{\Delta} - \Delta$ has a continuous distribution. (Assignment 2)
\[\tilde{\Delta} = Z_{\left(\frac{N+1}{2}\right)}\]

\bigskip \noindent
\textbf{Observe:} $\forall i \in \{1, \ldots, N\}$\\
\begin{align*}
Z_{(i)} \leq a &\Leftrightarrow \text{there are at least } i\; Z_i\text{'s} \leq a\\
&\Leftrightarrow \text{there are at most } N-i\; Z_i\text{'s} > a\\
&\Leftrightarrow \#\{j: Z_j - a >0\} \leq N-i\\
&\Leftrightarrow \sumjN \mathds{1}(Z_j - a >0) \leq N-i\\
&\Leftrightarrow S_n(Z-a) \leq N-i
\end{align*}
so we have that $\forall x \in \mathbb{R}$
\[\p{\sqrt{N}(\tilde{\Delta} - \Delta) \leq x)} \to \Phi(x2l(0))\]
since it goes to a normal distribution with mean 0 and variance $\frac{1}{4\{l(0)\}^2}$.
But we also have that the left hand side is equal to 
\[\mathbb{P}_\Delta\bigg(\underbrace{\tilde{\Delta}}_{Z_{\left(\frac{N+1}{2}\right)}} \leq \underbrace{\frac{x}{\sqrt{N}} + \Delta}_a\bigg) = \mathbb{P}_\Delta \bigg(S_N \left(z - \frac{x}{\sqrt{N}} - \Delta \right) \leq \underbrace{N - \frac{N+1}{2}}_{\frac{N-1}{2}}\bigg)\]
where
\begin{align*}
p&=\mathbb{P}_\Delta\left(Z - \Delta - \frac{x}{\sqrt{N}}\right)\\
&= \mathbb{P}_\Delta\left(Z - \Delta > \frac{x}{\sqrt{N}}\right)\\
&= 1 - L\left(\frac{x}{\sqrt{N}}\right)\\
&\approx \Phi\left(\frac{\frac{N-1}{2} + \frac{1}{2} - Np}{\sqrt{Np(1-p)}}\right)\\
&= \Phi\left(\frac{\sqrt{N}\left(\frac{1}{2} -p\right)}{\sqrt{p(1-p)}}\right)\\
&\approx \Phi(2x \l(0))
\end{align*}
Taking $N \to \infty$, and $p \approx 1/2$, $p(1-p) \approx 1/4$, this means
\begin{align*}
\frac{1}{2}-p &= \frac{1}{2} - 1 + L\left(\frac{x}{\sqrt{N}}\right)\\
&= L\left(\frac{x}{\sqrt{N}}\right) - L(0)\\
&\approx l(0) \frac{x}{\sqrt{N}}
\end{align*}

Now if 
\[\widehat{\Delta} : \frac{Z_i + Z_j}{2}\]
denote these averages by $A$ so we have
\[A_{(1)} \leq \ldots \leq A_{\left(\frac{N(N+1)}{2}\right)}\]
We have that
\begin{align*}
A_{(i)} \leq a &\Leftrightarrow \text{At least $i$ of $A_k$'s are} \leq a\\
&\Leftrightarrow \text{At most $\frac{N(N+1)}{2}=2$ of $A_k$'s satisfy} A_k >a\\
&\Leftrightarrow \text{At most $\frac{N(N+1)}{2}=2$ of $A_k$'s satisfy} A_k - a > 0\\
&\Leftrightarrow \sum_{i \leq j} \mathds{1}\left(\frac{Z_i + Z_j - 2a}{2}>0\right) \leq \frac{N(N+1)}{2}-i\\
&\Leftrightarrow \underbrace{\sum_{i \leq j}\mathds{1}\left(Z_i - a + Z_j - a >0\right)}_{V_S(Z-a)} \leq \frac{N(N+1)}{2} - i
\end{align*}
so
\begin{align*}
\mathbb{P}_\Delta \left(\widehat{\Delta} \leq \frac{x}{\sqrt{N}} + \Delta \right) &\underset{\frac{N(N+1)}{2}}{=} \mathbb{P}_\Delta \left(A_{\left(\frac{N(N+1)}{4} + \frac{1}{2}\right)} \leq \frac{x}{\sqrt{N}} + \Delta \right)\\
&= \mathbb{P}_\Delta\left(V_S \left(Z - \Delta - \frac{x}{\sqrt{N}}\right) \leq \frac{N(N+1)}{4} - \frac{1}{2}\right)
\end{align*}




\section{November 5th, 2012}
\subsection{Kruskal-Wallis Test (for several treatments)}
In our test, the null hypothesis $H_0$ is that the treatments are equivalent, so that there is no difference among the treatments. We label our observations as

\[\begin{array}{cc}
X_{11},\ldots,X_{1n_1} & \text{Group } 1\\
X_{21},\ldots,X_{2n_2} & \text{Group } 2\\
\vdots & \vdots\\
X_{s1},\ldots,X_{sn_s} & \text{Group } s\\
\end{array}\]
We pool the observations and then rank them.
\[R_{11},\ldots, R_{n_1}, R_{22},\ldots, R_{2n_2},R_{s1},\ldots, R_{sn_s}\] 

\begin{ex}
Assume that our ranks are 
\[\begin{array}{cc}
\text{Group } 1 & 2,4\\
\text{Group } 2 & 3,5,7\\
\text{Group } 3 & 1,6
\end{array} \]
with no ties. The number of distinct sums of the ranks in the groups is 
\[\frac{7!}{2!3!2!} = {7 \choose n_1 \; n_2 \; n_3}\]
\end{ex}
\subsection{Choosing an Alternative}
Our alternative is that each of the treatments is different from one another, so we look at the average of the ranks per group. Under the null, they should be close to each other.

\bigskip
\hrule
\bigskip

The Kruskal-Wallis Statistic is given by
\[K = \frac{12}{N(N+1)}\sum_{i=1}^s n_i \left(\bar{R}_{i\bullet} - \frac{N+1}{2}\right)^2\]
which tests $H_0$ against $H_1$ that there is a difference in location. Equivalently, we may take
\[W_i = R_{i1} + \ldots + R_{in_i}\]
so we have
\begin{align*}
\left(\bar{R}_{i\bullet} - \frac{N+1}{2}\right)^2 &= \left(\frac{1}{n_i}W_i - \frac{N+1}{2}\right)^2\\
&= \frac{W_i^2}{n_i^2} - (N+1)\frac{W_i}{n_i} + \frac{(N+1)^2}{4}
\end{align*}
so
\begin{align*}
K &= \frac{12}{N(N+1)}\sum_{i=1}^s \frac{W_i^2}{n_i} - \frac{12(N+1)}{N(N+1)}\sum_{i=1}^s W_i + \frac{12(N+1)^2}{4N(N+1)}\sum_{i=1}^s n_i\\
&= \frac{12}{N(N+1)}\sum_{i=1}^s \frac{W_i^2}{n_i} + \underbrace{\frac{12(N+1)}{\cancel{N(N+1)}}\frac{\cancel{N(N+1)}}{2} + \frac{12(N+1)^{\cancel{2}}}{4\cancel{N}\cancel{(N+1)}}\cancel{N}}_{-3(N+1)}\\
&=\frac{12}{N(N+1)}\sum_{i=1}^s \frac{W_i^2}{n_i}  - 3(N+1)
\end{align*}

\subsection{Asymptotic Approximation of the Kruskal-Wallis Statistic}
Assume that $s=2$. Then we have
\[K = \frac{12}{N(N+1)}\left(\frac{W_1^2}{n_1} + \frac{W_2^2}{n_2}\right) - 3(N+1)\]
We know that the $W_2$ is redundant since
\[W_1 + W_2 = \frac{N(N+1)}{2} \Leftrightarrow W_2 = \frac{N(N+1)}{2} - W_1\]
Now we know that
\[
\frac{W_1^2}{n_1} + \frac{W_2^2}{n_2} = \frac{W_1^2}{n_1} + \frac{N^2(N+1)^2}{4n_2} - \frac{N(N+1)W_1}{n_2} + \frac{W_1^2}{n_2}\]
so plugging this in, we have
\[K = \frac{12}{N(N+1)n_1n_2}\left(W_1^2\underbrace{(n_1 + n_2)}_{=N} - W_1N(N+1) - n_1 + \frac{N^2(N+1)^2n_1 - (N+1)^2N n_1 n_2}{4}\right)\]
and we have the last term is
\begin{align*}
\frac{N^2(N+1)^2n_1 - (N+1)^2N n_1 n_2}{4} &= \frac{(N+1)^2Nn_1\overbrace{(N-n_2)}^{n_1}}{4}\\
&= \frac{(N+1)^2Nn_1}{4}
\end{align*}
So substituting this, we get
\begin{align*}
K &= \underbrace{\frac{12}{(N+1)n_1n_2}}_{\displaystyle=\frac{1}{\Var(W_S)}}\bigg(W_1 - \underbrace{\frac{(N+1)n_1}{2}}_{\displaystyle \E{W_1}}\bigg)^2\\
&= \left(\frac{W_1 - \E{W_1}}{\sqrt{\Var(W_1)}}\right)^2\\
&\sim \chi^2_1
\end{align*}
For general $s$, 
\begin{align*}
K &= \sum_{i=1}^s \frac{12}{N(N+1)}n_i\left(\frac{W_i}{n_i} - \frac{N+1}{2}\right)^2 \\
&= \sum_{i=1}^s \frac{12}{N(N+1)n_i}\bigg(W_i - \underbrace{\frac{n_i(N+1)}{2}}_{\E{W_i}}\bigg)^2\\
&= \frac{1}{N}\sum_{i=1}^s(N-n_i) \left(\frac{W_i - \E{W_i}}{\sqrt{\Var(W_i)}}\right)^2\\
&\leadsto \chi^2_{s-1}
\end{align*}
Since knowing $s-1$ of the $W_i$'s gives you the last one, they are redundant and hence not independent.

\subsection{Dealing with Ties}
Our altered statistic is given by
\[K^* = \frac{1}{N}\sum_{i=1}^s(N-n_i) \left(\frac{W_i^* - \E{W_i}}{\sqrt{\Var(W_i^*)}}\right)^2\]
where
\[\Var(W_i^*) = \frac{(N+1)(N-n_i)n_i}{12}\left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N}\right)\]
so
\begin{align*}
K^* &= \frac{1}{N}\sum_{i=1}^s \frac{(N-n_i)\left(W_i^* - \frac{n_i(N+1)}{2}\right)^2}{\frac{(N+1)n_i(N-n_i)}{12} \cdot \left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N}\right)}\\
&= \left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N} \right)K\\
&= \left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N} \right) \left(\frac{12}{N(N+1)}\sum_{i=1}^s \frac{W_i^2}{n_i} - 3(N+1)\right)
\end{align*}


\section{November 7th, 2012}
\begin{align*}
K^* &= \frac{1}{N}\sum_{i=1}^s \frac{(N-n_i)\left(W_i^* - \frac{n_i(N+1)}{2}\right)^2}{\frac{(N+1)n_i(N-n_i)}{12} \cdot \left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N}\right)}\\
&= \frac{\frac{12}{N(N+1)}\sum_{i=1}^s \frac{(W_i^*)^2}{n_i} - 3(N+1)}{\left(1 - \frac{\sum_{j=1}^l d_j^3 - d_j}{N^3 - N}\right)}
\end{align*}

\bigskip 
\begin{center}
\begin{tabular}{cc|ccc|c}
\hline
 & & ++ & + & 0 & \\
 \hline
 Tylenol & A & 2 & 1 & 2 & 5\\
 Advil & B & 3 & 2 & 0 & 5\\
\hline
\end{tabular}
\end{center}
\bigskip
We could assign 2 to $++$, 1 to $+$ and 0 to 0 and use a Wilcoxon test for this. So our values are 
\begin{align*}
A&: \; 0 \; 0 \; 1 \; 2 \; 2\\
B&: \; 2 \; 2\; 2 \; 1 \;1 
\end{align*}
and pooling the values, we have the pooled midranks given by
\[\begin{array}{ccccccccccc}
\hline
\text{Values}&0 & 0 & 1 & 1 & 1  & 2 & 2 & 2 & 2 & 2\\
\text{Mid-ranks}&1.5 & 1.5 & 4 & 4 & 4 & 8 & 8 & 8 & 8 & 8 \\
\hline
\end{array} \]
We have in a standard contingency table, the standard notation
\[\begin{array}{c|c|c|c}
\hline
& \text{``0''} & \text{``1''} & \\ \hline
1 & & & n_{\bullet 1}\\ \hline
\vdots & & & \vdots\\ \hline
i &n_{1i} &n_{2i}& n_{\bullet i}\\ \hline
\vdots & & & \vdots\\ \hline
t & & & n_{\bullet t}\\ \hline
& n_{1 \bullet} & n_{2 \bullet} & n_{\bullet \bullet}
\end{array}\]
Then
\[R_1^* = \frac{n_{1 \bullet} + 1}{2} \qquad R_2^* = n_{1 \bullet} + \frac{n_{2 \bullet} + 1}{2}\]
\bigskip
\hrule
\bigskip
All this stuff I should check since it's likely I made a typo
\begin{align*}
\left(1 - \frac{n_{1\bullet}^3 - n_{1 \bullet} - n_{2 \bullet}^3 - n_{2 \bullet}}{n_{\bullet \bullet}^3 - n_{\bullet \bullet}}\right) &= \frac{n_{\bullet \bullet}^3 - n_{1 \bullet}^3 - n_{2 \bullet}^3}{n_{\bullet \bullet}^3 - n_{\bullet \bullet}}\\
&= \frac{3n_{1\bullet} n_{2 \bullet} \cancel{n_{\bullet \bullet}}}{\cancel{n_{\bullet \bullet}}(n_{\bullet \bullet}^2-1)}
\end{align*}
and
\begin{align*}
n_{\bullet \bullet}^3 &= (n_{1 \bullet} + n_{2 \bullet})^3\\
&= n_{1 \bullet}^3 + 3n_{1 \bullet}^2n_{2 \bullet} + 3 n_{1 \bullet} n_{2 \bullet}^2 + n_{2 \bullet}^3
\end{align*}
The Wilcoxon statistics are given by
\begin{align*}
& \quad \; W_i^* - \frac{n_{\bullet i}(n_{\bullet \bullet} + 1)}{2}\\
&= n_{1i}\left(\frac{n_{1 \bullet} + 1}{2}\right) + n_{2i} \left(n_{1 \bullet} + \frac{n_{2 \bullet} + 1}{2}\right)\\
&= \frac{n_{1i}n_{1 \bullet} + n_{1i} + 2n_{2i}n_{1 \bullet} + n_{2i}n_{2 \bullet} + n_{2i} - n_{\bullet i} - n_{\bullet i}n_{\bullet \bullet}}{2}\\
&=\frac{n_{1i}n_{1 \bullet} + 2n_{2i}n_{1 \bullet} + n_{2i}n_{2 \bullet} + n_{1i} n_{2 \bullet} - n_{2i}n_{1 \bullet} - n_{2i}n_{2 \bullet}}{2}\\
&= \frac{n_{2i}n_{1 \bullet}}{2} - \frac{n_{1 i} n_{2 \bullet}}{2}
\end{align*}
\bigskip
\hrule
\bigskip
\begin{align*}
\bar{K}^* &= \frac{1}{n_{\bullet \bullet}} \sum_{i=1}^t(n_{\bullet \bullet} - n_{\bullet i}) \frac{(n_{2i}n_{1 \bullet} - n_{1i}n_{2 \bullet})12}{4(n_{\bullet \bullet} + 1)n_{i \bullet}\frac{3n_{1\bullet} n_{2 \bullet}}{n_{\bullet \bullet}^2 - 1}}\\
&= \frac{n_{\bullet \bullet}-1}{n_{\bullet\bullet}} \sum_{i=1}^t \frac{(n_{2i}n_{1 \bullet} - n_{1i}n_{2 \bullet})^2}{n_{1 \bullet}n_{2 \bullet} n_{\bullet i}}
\end{align*}

\subsection{General Formula}
We have that $B_i = n_i - A_i$ $(n_{\bullet i } - n_{1 i})$, so 
\begin{align*}
W_i^* &= R_i^*\\
&= A_i \left(\frac{m+1}{2} -m - \frac{n+1}{2}\right) + n_i\left(m + \frac{n+1}{2}\right)\\
&= A_i\left(- \frac{N}{2}\right) + n_i \left(\frac{m+N+1}{2}\right)\\
\end{align*}
and so
\begin{align*}
\sum \frac{(W_i^*)^2}{n_i} = \frac{N^2}{4}\sum \frac{A_i^2}{n_i} - \frac{N}{2}(m+N+1) \underbrace{\sum_{i=1}^t A_i}_m  + \left(\frac{m+N+1}{2}\right)^2\underbrace{\sum_{i=1}^t n_i}_N
\end{align*}



\section{November 12th, 2012}
\subsection{The Jonckheere Test}
Suppose we have three diets we would like to compare

\[\begin{array}{c|ccccc}
\text{Diet A} & 133 & 139 & 149 & 160 & 184\\
\hline
\text{Diet B} & 111 & 125 & 143 & 148 & 157\\
\hline
\text{Diet C} & 99 & 114 & 116 & 127 & 146
\end{array} \]
If we wish to test
\begin{align*}
H_0: & \quad\text{no difference between the diets}\\
H_1: & \quad\text{``general''}
\end{align*}
we must first rank them
\[\begin{array}{c|ccccc}
\text{Diet A} & 7 & 8 & 12 & 14 & 15\\
\hline
\text{Diet B} & 2 & 5 & 9 & 11 & 13\\
\hline
\text{Diet C} & 1 & 3 & 4 & 6 & 10
\end{array} \]
and we look at the Kruskal-Wallis statistic given by
\[K = \frac{12}{N(N+1)}\sum_{i=1}^3 \underbrace{n_i}_5 \left(\bar{R}_{i \bullet} - \frac{N+1}{2}\right)^2\]
We let
\[W_{XY} = \sumin \left\{\mathds{1}(Y_i > X_j) + \frac{1}{2}\mathds{1}(Y_i = X_j)\right\}\]
For Jonckheere's test, we must assume apriori that we know the ordering, that is
\[H_1^*: \quad A > B> C\]
We look at various Mann-Whitney statistics and expect,
$W_{BA}$, $W_{CA}$, and $W_{CB}$ to be large. Our statistic is defined as
\[W = W_{BA} + W_{CA} + W_{CB} \]

\[\begin{array}{c|c}
\hline
W_{BA} & 2 + 2 + 4 + 5 + 5 = 18\\
W_{CA} &4 + 4 + 5 + 5 + 5 = 23 \\
W_{CB} & 1 + 3 + 4 + 5 + 5 =18\\
\hline
\end{array} \]

Now to get the expected value, we look at 
\[N = \sum_{i=1}^s n_i\]
which gives us
\begin{align*}
N^2 &= \left(\sum_{i=1}^s n_i\right)^2\\
&= \sum_{i=1}^s n_i^2 + \underbrace{\sum_{\substack{i,j=1\\i \neq j}}^s n_i n_j}_{\displaystyle 2 \sum_{i < j} n_i n_j}
\end{align*}
but we have
\[\frac{1}{2}\sum_{i<j} n_i n_j = \frac{(N^2 - \sum n_i^2)}{4}\]


The variance? (check this) is given by
\[\frac{1}{72}\left(N^2(2N+3) - \sum_{i=1}^s n_i^2 (2n_i+3)\right)\]

\subsection{Friedman's Test}

\[\begin{array}{c|c|c|c|c|c}
 & 1 & 2 & 3 & 4 & 5\\
 \hline
A &3 &2 &3 &1 &2\\
\hline
B &2 & 3& 1& 2& 3\\
\hline
C &1 &1 &2 &3 &1\\
\hline
\end{array} \]
In each column, we have $3!=6$ ways of permuting the ranks, so in total we have $(3!)^5$ possible configurations. Now we sum the ranks.
\begin{align*}
R_{1\bullet} &= 3 + 2 + 3 + 1 + 2 = 11\\
R_{2\bullet} &= 2 + 3 + 1 + 2 + 3 = 11\\
R_{3\bullet} &= 1 + 1 + 2 + 3 + 1 = 8
\end{align*}
and the averages are given by
\begin{align*}
\bar{R}_{1\bullet} &= \frac{11}{5}\\
\bar{R}_{2\bullet} &= \frac{11}{5}\\
\bar{R}_{3\bullet} &= \frac{8}{5}
\end{align*}

However, the sum of all these ranks is different. In the Kruskal-Wallis case, we had
\[\text{K-W}: \quad \frac{N(N+1)}{2N} = \frac{N+1}{2}\]
and
\[\frac{12}{N(N+1)} \sum n_i \left(\bar{R}_{i\bullet} - \frac{N+1}{2}\right)^2\]
but for the Friedman, we have
\[\text{F}: \quad \frac{ns(s+1)}{2ns} = \frac{s+1}{2}\]
and 
\[\frac{12}{N(N+1)} \sum n_i \left(\bar{R}_{i\bullet} - \frac{s+1}{2}\right)^2\]
but we have that for both these tests, these two statistics under $H_0$ are $\chi^2_{(s-1)}$.


\section{November 14th, 2012}
Suppose we would like to investigate 3 tranquilizers. Within each column, the results are not independent since they are acting on the same body.

\[\begin{array}{c|r|r|r|r}
  & 1 & 2 & 3 & 4\\
  \hline
A & +++ &++  & +++ & +++\\
\hline
B & ++ & +++ & + & + \\
\hline
C & +  & + & ++ & ++\\
\end{array} \]
Computing the ranks, we get

\[\begin{array}{c|r|r|r|r||l}
  & 1 & 2 & 3 & 4 & \\
  \hline
A & 3 & 2  & 3 & 3 & \bar{R}_{1\bullet}=11 \\
\hline
B & 2 & 3 & 1 & 1 & \bar{R}_{2\bullet}=7  \\
\hline
C & 1  & 1 & 2 & 2  & \bar{R}_{3\bullet}=6\\
\end{array} \]
This gives us that
\begin{align*}
Q &= \frac{12n}{s(s+1)}\sum_{i=1}^s \left(\bar{R}_{i \bullet} - \frac{s+1}{2}\right)^2\\
&= \frac{12 \cdot 4}{3 \cdot 4} \left\{\left(\frac{11}{4} - \frac{4}{2}\right)^2 + \left(\frac{7}{4} - \frac{4}{2}\right)^2 + \left(\frac{6}{4} - \frac{4}{2}\right)^2\right\}
\end{align*}
The $p$-value is given by $\mathbb{P}_0(Q \geq q)$, which we compute using the $\chi^2$ approximation.

If we look at our statistic, $Q^*$, we have that the numerator is
\[\frac{12}{sn(s+1)}\sum_{i=1}^s (R_{i\bullet}^*)^2 - 3n(s+1)\]
To get the denominator we first consider each block.
\begin{align*}
\Var(R_{\bullet j}^*) &= \Var(R_{1j}^* + \cdots + R_{sj}^*)\\
&=\frac{s^2-1}{12} - \frac{\sum_{i=1}^{l_j} (d_{ij}^3 - d_{ij}}{12 \cdot s}
\end{align*}
We have
\begin{align*}
&\quad \; 1 - \frac{1}{ns(s^2-1)}\sum_{j=1}^n \left\{s(s^2-1) - \Var(r_{\bullet j}^*)12s\right\}\\
&= 1 - 1 + \frac{12s}{ns(s^2-1)}\sum_{j=1}^n \Var(R_{\bullet j}^*)
\end{align*}
which gives us
\[Q^* = \frac{\frac{12n}{s(s+1)}\frac{1}{n^2}\sum_{i=1}^s \left(R_{1\bullet}^* - \frac{(s+1)n}{2}\right)^2}{\frac{12n}{ns(s^2-1)}\sum_{j=1}^n \Var(R_{\bullet j}^*)}\]

\subsection{A special case (s=2)}
We have two treatments $A$ and $B$ and for each subject there is a preference, so we may have ranks 1 for $A$ and 2 for $B$ or 1 for $B$ and 2 for $A$. Let $A$ be the number of ones in row 1. Then there are $n-A$ twos in row 1. Similarly, the number of ones in row 2 is $n-A$ and the number of twos is $A$.

Let $L_j$ be the number of successes in block $j$. This means there are $s-L_j$ zeroes. $B_i$ is the total number of successes for treatment $i$. The mid-ranks for the zeros is given by
\[\frac{1 + \cdots + (s - L_j)}{s-L_j} = \frac{s-L_j + 1}{2} = \frac{s+1}{2} - \frac{L_j}{2}\]
For the ones, the mid-ranks is
\begin{align*}
\frac{(s-L_j+1) + \cdots + s}{L_j} &= \frac{L_j(s - L_j)}{L_j} + \frac{1 + \cdots + L_j}{L_j}\\
&= s - L_j + \frac{L_j+1}{2}\\
&= s + \frac{1}{2} - \frac{L_j}{2} 
\end{align*}
This means that 
\begin{align*}
R_{i\bullet}^* &= \sumjn R_{ij}^*\\
&= \sum_{j: ``0''}\left(\frac{s+1}{2} - \frac{L_j}{2}\right) + \sum_{j: ``1''}\left(s + \frac{1}{2} - \frac{L_j}{2}\right)\\
&= \frac{s+1}{2}(n-B_i) + \left(s + \frac{1}{2}\right)B_i - \frac{1}{2}\sumjn L_j\\
&= \sumin B_i
\end{align*}
The numerator is equal to
\[\frac{3s}{n(s+1)}\sumin B_i^2 - \frac{3}{n(s+1)}\left(\sumin B_i\right)^2\]
The denominator is
\begin{align*}
&\quad \;1 - \frac{1}{ns(s^2-1)}\sumjn \sumin (d_{ij}^3 - d_{ij})\\
&= 1 - \frac{1}{ns(s^2-1)} \sumjn \left\{(s-L_j)^3 - (s - L_j) + L_j^3 - L_j \right\}
\end{align*}


\section{November 19th, 2012}
\subsection{Cochran and McNemar tests}
If we look at the $Q^*$ statistic and consider $s=2$, then the numerator becomes
\[2B_1^2 + 2 B_2^2 - (B_1 + B_2)^2 = (B_1 - B_2)^2\]
and the denominator is 
\[\sumjN L_j(2 - L_j)\]
If $s=2$ and the data are dichotomous, we only need to care about the number of the pairs $(0,0), (0,1), (1,0), (1,1)$, so we can summarize this in a $2 \times 2$ table.
\[\begin{array}{cc|cc}
& & \multicolumn{2}{c}{\text{Tr 1}}\\
& & 0 & 1\\
\hline
\multirow{2}{*}{\text{Tr 2}} & 0 & A & B\\
& 1 & C & D
\end{array}\]
The numerator is equal to 
\[(\underbrace{C+D}_{B_1} - \underbrace{B-D}_{B_2}) = (C-B)^2\]
and the denominator is equal to
\begin{align*}
A\cdot 0(2-0) + (B+C)1(2-1) + D \cdot 2(2-2) &= 0 + B+C +0\\
&= B+C
\end{align*}
so
\[Q^* = \frac{(C-B)^2}{B+C}\]
Here we have $B+C = N_+ =k$ which is equal to the number of non-zero differences.
Now suppose we have
\begin{align*}
H_0: & \; \text{no difference between the treatments}\\
H_1: & \; \text{Treatment 2 is better (one-sided)}\\
& \text{or}\\
H_1: & \; \text{Treatment is either better or worse (two-sided)}
\end{align*}
Under $H_0$, $B \sim \text{Bin}(k,1/2)$.
In the one-sided case, the $p$-value is 
\[\p{B \geq B_{obs}}\]
and in the two-sided case, the $p$-value is
\[\p{\left|B - \frac{k}{2}\right| \geq \left|B_{obs} - \frac{k}{2}\right|}\]
Another way to look at this is
\[B - \frac{B+C}{2} = \frac{2B-B-C}{2} = \frac{B-C}{2}\]
so
\[\p{\frac{|B-C|}{k} \geq \frac{|B_{obs} - C_{obs}|}{k}} = \p{Q^* \geq Q^*_{obs}}\]

\subsection{Aligned Rank Tests}
With the Wilcoxon sign-ranked test, we had the assumption that under the null, the differences $Z_1, \ldots, Z_N$ were iid and symmetric about 0, that is $(X,Y) \overset{d}{=} (Y,X)$. For the sign-test, we required independence, not identically distributed.

\[\begin{array}{c|c|c|c}
& 1 & \cdots & N\\
\hline
\text{Tr 1} & & &\\
\hline
\vdots & & & \\
\hline
\text{Tr }s & & &
\end{array}
\]
Now we have a slightly difference exchangeability condition. We have
\[H_0: (X_{1i}, \ldots, X_{si}) \qquad \text{iid}\]
and our exchangeability condition is given by
\[(X_{1i}, \ldots, X_{si}) \overset{d}{=} (X_{\pi(1)i}, \ldots, X_{\pi(s)i})\]
for any permutation $\pi(1), \ldots, \pi(s)$ of $1, \ldots, s$.

\subsection{Testing for Trends}
Let us assume that we have
\[H_0: \; X_1 \overset{d}{=} X_2 \overset{d}{=} \cdots \overset{d}{=}X_N \quad \text{and are independent}\]
In the case where $N=5$.
\[\begin{array}{c|c|c|c|c}
1 & 2 & 3 & 4& 5\\
\hline
R_1 & R_2 & R_3 & R_4 & R_5
\end{array} \]
The most upward trend occurs when $R_1 = 1, \ldots R_5 = 5$.
There are $N!$ possible orderings. Let us look at a baby example, when $N=3$.
\[\begin{array}{c|c|c|c}
1 & 2 & 3 & D\\
\hline
1 & 2 & 3 & 0\\
1 & 3 & 2 & 2\\
2 & 1 & 3 & 2\\
2 & 3 & 1 & 6\\
3 & 1 & 2 & 6\\
3 & 2 & 1 & 8
\end{array} \]
where
\[D = \sumiN (R_i - i)^2\]
and so the distribution of $D$ is
\[\p{D=d} =\begin{cases}
0 & \text{with prob.} \frac{1}{6}\\
2 & \text{with prob.} \frac{2}{6}\\
6 & \text{with prob.} \frac{2}{6}\\
8 & \text{with prob.} \frac{1}{6}
\end{cases} \]
So if we observe $3\;1\;2$ then our $p$-value is given by
\begin{align*}
\p{D \leq D_{obs}} &= \p{D \leq 6}\\
&= \frac{5}{6}
\end{align*}

\bigskip
\hrule
\bigskip
\begin{align*}
\E{R_i} &= \sumiN i \frac{1}{N}\\
&= \frac{1}{N} \frac{N(N+1)}{2}\\
&= \frac{N+1}{2}
\end{align*}
The variance is given by
\begin{align*}
\Var(D) &= 4 \Var\left(\sumiN i R_i\right)\\
&= 4 \left(\sumiN i^2 \Var(R_i) + \sum_{i \neq j} ij \Cov(R_i,R_j)\right)\\
\end{align*}
To calculate the variance, we use
\begin{align*}
\E{R_i^2} &= \sumiN \frac{1}{N} i^2\\
&= \frac{N(N+1)(2N+1)}{6N}\\
&= \frac{(N+1)(2N+1)}{6}
\end{align*}
So we have
\begin{align*}
\Var(R_i) &= \frac{(N+1)(2N+1)}{6} - \frac{(N+1)^2}{4}\\
&= \frac{(N+1)(2N+1-3N-3)}{12}\\
&= \frac{(N+1)(N-1)}{12}
\end{align*}
To compute the covariance, we have
\begin{align*}
\Cov(R_i,R_j) &= \underbrace{\E{R_i R_j}}_{\sum_{i \neq j} ij\frac{1}{N(N-1)}} - \underbrace{\E{R_i}\E{R_j}}_{\frac{(N+1)^2}{4}}\\
&= -\frac{N+1}{12}
\end{align*}


\section{November 21st, 2012}
\[D = \|(1,\ldots,N) - (R_1, \ldots, R_N)\|^2_2\]
Spearman's Footrule\\
\[\sumiN |R_i -i|\]
and
\[D = \sumiN (R_i - i)^2\]
with
\[\E{D} = \frac{N^3-N}{6}\qquad \Var(D) = \frac{N^2(N+1)^2(N-1)}{36}\]

Consider the worse case for an increasing trend, that is when the ranks are $N, N-1, \ldots, 1$.
\begin{align*}
D &= \sumiN ((N-i + 1) - i)^2\\
&= \sumiN ((N+1) - 2i)^2\\
&= (N+1)^2 N - 4(N+1)\sumiN i + 4 \sumiN i^2\\
&= (N+1)^2 N - 2(N+1)^2 N + 4\frac{N(N+1)(2N+1)}{6}\\
&= \frac{2N(N+1)(2N+1)}{3} - (N+1)^2N\\
&= \frac{N(N+1)(4N+2 - 3N - 3)}{3}\\
&= \frac{N(N+1)(N-1)}{3}
\end{align*}
so $D$ is small if 
\[\frac{N(N^2-1)}{3} - D\]
is large, which is the same if
\[D'=\frac{N(N^2-1)}{6} - \frac{1}{2}D\]
is large.

\bigskip
\hrule
\bigskip

\begin{align*}
R_j - 1&= \sum_{i < j} \mathds{1}(R_i < R_j) + \sum_{i > j} \mathds{1}(R_i < R_j)\\
&= \sum_{i < j} U_{ij} + \sum_{i > j} U_{ij}
\end{align*}
Multiplying by $j$ and summing over all $j$, 
\begin{align*}
\sumjN j(R_j-1) &= \sumjN \sum_{i=1}^{j-1} j U_{ij} + \sumjN \sum_{i=1}^{j-1} jU_{ij}\\
&= \sumjN \sum_{i=1}^{j-1} j U_{ij} + \sumiN \sum_{j=1}^{i-1} j U_{ij}\\
&= \sumjN \sum_{i < j} j U_{ij} + i \underbrace{U_{ji}}_{1 - U_{ij}}
\end{align*}
So this gives us
\[\sumjN j R_j - \frac{N(N+1)}{2} = \sumjN \sum_{i < j} (j-i)U_{ij} + \sum_{j=1}^N \sum_{i < j} i\]
but
\begin{align*}
\sum_{j=1}^N \sum_{i < j} i &= \sumiN \sum_{j > i} i\\
&= \sumiN (N-i)i
\end{align*}

\bigskip
\hrule
\bigskip

\[B = \sum_{i < j} U_{ij}\]
Its expectation is given by
\begin{align*}
\E{B} &= \sum_{i < j} \E{U_{ij}}\\
&= \sum_{i < j} \E{\mathds{1}(X_i < X_j)}\\
&= {N \choose 2} \p{X_1 < X_2}\\
&= \frac{N(N-1)}{4}
\end{align*}
The variance is given by
\begin{align*}
\Var(B) &= \sum_{i < j} \Var(\mathds{1}(X_i < X_j)) + \sum_{\substack{i<j,k<l\\(i,j) \neq (k,l)}} \Cov(\mathds{1}(X_i<X_j),\mathds{1}(X_k<X_l))
\end{align*}


\subsection{Tests of Independence}
\begin{align*}
B &= \sum_{i < j} \mathds{1}(S_i < S_j) \\
&= \sum_{i  <j} \mathds{1}(S_i < S_j) \mathds{1}(i<j)\\
&= \sum_{\substack{\text{all pairs}\\(R_i,S_i),(R_j,S_j), i \neq j}} 
\mathds{1}(S_i < S_j) \mathds{1}(R_i < R_j) + \mathds{1}(S_j < S_i) \mathds{1}(R_j < R_i)
\end{align*}
Given $(a_1,b_1)$ and $(a_2,b_2)$, we say they are concordant if $a_1<a_2$ and $b_1<b_2$ or $a_2<a_1$ and $b_2<b_1$. They are discordant if $a_1<a_2$ and $b_1 > b_2$ or $a_2 < a_1$ and $b_1 < b_2$. 

\bigskip

\begin{align*}
\tau_N &= \text{Kendall's tau}\\
&= \frac{\sum_{\text{all pairs of points}} \mathds{1}(\text{concordant}) - \mathds{1}(\text{discordant})}{{N \choose 2}}\\
&= \frac{B-C}{{N \choose 2}}\\
&= \frac{B - {N \choose 2} + B}{{N \choose 2}}
\end{align*}

\section{November 26th, 2012}
\[D = \sumin (R_i - S_i)^2\]
We have that
\[\E{D} = \frac{N^3 - N}{6} \qquad \Var(D) = \frac{N^2(N+1)^2(N-1)}{36}\]
if $X \indep Y$.
\begin{align*}
B &= \sum_{i < j} U_{ij}\\
&=\sum_{i<j} \mathds{1}((R_i - R_j)(S_i - S_j) > 0)\\
&= \text{number of concordant pairs}
\end{align*}
Under $X \indep Y$, 
\[\E{B} = \frac{N(N-1)}{4} \qquad \Var(B) = \frac{2N^3 - 3N^2 - 5N}{72}\]

\bigskip
\hrule
\bigskip
\[\tau_N = \frac{2B}{{N \choose 2}}-1 = \frac{4B}{N(N-1)} - 1\]
Under the hypothesis of independent, we have
\[\E{\tau_N} = \frac{4}{N(N-1)}\E{B} - 1 =0\]
and 
\begin{align*}
\Var(\tau_N) &= \frac{16 \cdot \Var(B)}{N^2(N-1)^2}\\
&= \frac{16N(2N^2 + 3N - 5)}{72 N^2(N-1)^2}\\
&= \frac{2(2N+5)(N-1)}{9N(N-1)^2}\\
&= \frac{2(2N+5)}{9N(N-1)}
\end{align*}

\bigskip
\hrule
\bigskip
To compare $D$ to $\rho_N$, we note
\begin{align*}
\sumiN (R_i - \bar{R})^2 &= \sumiN \left(i - \frac{N+1}{2}\right)^2\\
&= \frac{N^3 - N}{12}
\end{align*}
and
\begin{align*}
\sumiN R_i S_i &= \frac{N(N+1)(2N+1)}{6} - \frac{D}{2}\\
S_N &= \frac{\frac{N(N+1)(2N+1)}{6} - \frac{D}{2} - \frac{N(N+1)^2}{4}}{\frac{N^3-N}{12}}\\
&= \frac{-6D}{N^3 - N} + \frac{\frac{N(N+1)(4N+2-3N-3)}{12}}{\frac{N(N-1)(N+1)}{12}}\\
&= 1 - \frac{6D}{N^3 - N}
\end{align*}

If we have $(X_1,Y_1) \indep (X_2,Y_2) \sim H$, then
\[\mathds{1}((X_1-X_2)(Y_1-Y_2) >0) = \begin{cases}
1 & \text{concordance}\\
0 & \text{discordance}
\end{cases}\]
and
\[\mathds{1}((X_1-X_2)(Y_1-Y_2) <0) = \begin{cases}
1 & \text{discordance}\\
0 & \text{concordance}
\end{cases}\]
so
\begin{align*}
\tau &= 2\p{\text{concordance}}-1\\
&= \p{\text{concordance}} - (1 - \p{\text{concordance}})\\
&= \p{\text{concordance}} - \p{\text{discordance}}
\end{align*}
To get the probability of concordance, we have
\begin{align*}
\p{\text{concordance}} &= \p{X_1<X_2,Y_1<Y_2} + \p{X_2<X_1,Y_2<Y_1}\\
&= 2\p{X_1<X_2,Y_1<Y_2}\\
&= 2 \int \! H(x_2,y_2) \, \mathrm{d}H(x_2,y_2)
\end{align*}

\bigskip
\hrule
\bigskip
When looking at Spearman's rho, to calculate $I_n$, we use the fact that
\begin{align*}
R_i &= \sumjN \mathds{1}(X_k \leq X_i)\\
S_i &= \sum_{k=1}^N \mathds{1}(Y_k \leq Y_i)
\end{align*}

The value $\p{X_j \leq X_i, Y_j \leq Y_j}$ is one half of the probability of concordance.

\begin{align*}
\E{\rho_N} &= \frac{12}{N(N+1)(N-1)}\bigg\{N(N-1)(N-2)\frac{\rho+3}{12}\\
&\quad + 2N(N-1)\frac{1}{2} + N(N-1)\frac{\tau+1}{4} + N \bigg\} - 3\frac{N+1}{N-1}
\end{align*}


\section{November 28th, 2012}
The Pearson test is not good to use when normality does not hold. A good indication of this is when there are outliers.

\begin{align*}
\rho &- -3 + 12 \E{F(X)G(Y)}\\
&= \Corr(F(X),G(Y))
\end{align*}
and
\[\tau = -1 + 4 \E{H(X,Y)}\]
where $H$ is the underlying CDF of $(X_i,Y_i)$, $(X,Y) \sim H$. In a bivariate CDF $H$ of $(X,Y)$, 
\[F(x) = \p{X\leq x} = \lim_{y \to \infty} H(x,y)\]
We have that
\[H(x,y) = C_\theta(F(x),G(y))\]
where $C_\theta$ is some CDF on $[0,1]^2$ with $\mathcal{U}(0,1)$ margins. $C_\theta$ is called a ``Copula''. 

\bigskip

\begin{theorem}[Sklar's Representation Theorem]
Let $(X,Y)$ be a random pair with CDF $H$. Then there exists at least one copula $C$ such that 
\[H(x,y) = C(F(x),G(y)) \quad x,y \in \mathbb{R}\]
where $F(x)$ and $G(y)$ are the margins of $X$ and $Y$. If $F$ and $G$ are continuous, then $C$ is unique and it happens to be the joint distribution function of $(F(X),G(Y))$.
\end{theorem}

\[\rho = -3 + 12 \int_0^1 \int_0^1 \! uv \, \mathrm{d}C(u,v)\]
and we would like to apply
\[W(u,v) \leq C(u,v) \leq M(u,v)\]
which are the Fr\'echet Hoeffding lower and upper bounds.

\bigskip

If we take $(U,V) \sim C$, where $U^* \sim \mathcal{U}(0,1)$, $V^* \sim \mathcal{U}(0,1)$ and $(U,V) \indep U^* \indep V^*$
\begin{align*}
\p{U^* \leq u, V^* \leq v} &= \int_0^1 \int_0^1 \! uv \, \mathrm{d}C(u,v)\\
\end{align*}
We see that 
\begin{align*}
\p{U^* \leq U, V^* \leq V} &= \p{U^* \leq U} + \p{U^* \leq U, V \leq V^*}\\
&=\underbrace{\p{U^* \leq U}}_{=1/2} - \underbrace{\p{V \leq V^*}}_{=1/2} + \p{U \leq U^*, V \leq V^*}\\
&= \p{U \leq U^*, V \leq V^*}
\end{align*}
and so
\[\p{U \leq U^*,V \leq V^*} = \int_0^1 \int_0^1 \! C(u^*,v^*) \du^* \dv^*\]
so we have that
\begin{align*}
\rho &= -3 + 12 \int_0^1 \int_0^1 \! C(u,v) \du \dv\\
&\leq -3 + 12 \int_0^1 \int_0^1 \! M(u,v) \du \dv
\end{align*}
and
\[\rho \geq -3 + 12 \int_0^1 \int_0^1 \! W(u,v) \du \dv\]

Now for 
\[\rho = \Corr(\underbrace{F(X)}_U,\underbrace{G(Y)}_V)\]
if $U = V$, then $\Corr(U,U) = 1$ and if $V = 1-U$, we have $\Corr(U,1-U) = -1$.

\bigskip
\hrule
\bigskip
Consider $(X_1,Y_1), \ldots ,(X_n,Y_n)$. If we knew $F$ and $G$ we could plot 
\[(F(X_i),G(Y_i) \quad 1 \leq i \leq n\]
If $F$ and $G$ are unknown, plot 
\[(F_n(X_i),G_n(Y_i))\] 
instead
\[F_n(X_i) = \frac{1}{n}\sumjn \mathds{1}_{X_j \leq X_i} = \frac{R_i}{n}\]
Similarly, 
\[G_n(X_i) = \frac{S_i}{n}\]
So we have that
\[(F_n(X_i),G_n(Y_i)) \approx \left(\frac{R_i}{n+1},\frac{S_i}{n+1}\right)\]



\section{December 3rd, 2012}
\noindent
\textbf{Final:}
\begin{itemize}
\item Chapter 1
\item Chapter 2 (subsections 1-5)
\item Chapter 3 (subsections 1-3)
\item Chapter 4 (subsections 1-4)
\item Chapter 5 (subsections 1-5)
\item Chapter 6 (subsections 1-3)
\item Chapter 7 (subsections 1-3)
\end{itemize}
Everything on copulas will \textbf{NOT} be on the final.

\bigskip

Sklar's Decomposition
\[H(x,y) = C(F(x),G(y))\]
$C$ is a copula, which is a CDF with standard uniform margins. If $F$ and $G$ are continuous, then $C$ is unique and it is the CDF of $(F(X),G(Y))$.

We have $(X_1,Y_1),\ldots,(X_n,Y_n)$ are from $H$.
\[X \indep Y \Leftrightarrow C(u,v) = uv\]
If we knew the margins, we could look at 
\[(F_n(X_1),G_n(Y_1)),\ldots, (F_n(X_n),G_n(Y_n))\]
We deduced last class that
\[F_n(X_i) = \frac{R_i}{n} \qquad G_n(X_i) = \frac{S_i}{n}\]

\begin{align*}
\tau &= -1 + 4 \int_0^1 \int_0^1 \! C(u,v) \, \mathrm{d}C(u,v)\\
\rho &= -3 + 12 \int_0^1 \int_0^1 \! C(u,v) \du \dv
\end{align*}
and
\[\sumin \frac{R_i}{n+1}\frac{S_i}{n+1} = \sumin J\left(\frac{R_i}{n+1},\frac{S_i}{n+1}\right)\]
where $J(u,v) = uv$. We have that our statistic is 
\begin{align*}
V_n &= \sumiN \Phi^{-1}\left(\frac{R_i}{N+1}\right) \Phi^{-1} \left(\frac{S_i}{N+1}\right)\\
&= \sumiN \Phi^{-1}\left(\frac{i}{N+1}\right)\Phi^{-1}\left(\frac{Q_i}{N+1}\right)
\end{align*}
and
\[\E{V_n} = \sumiN \Phi^{-1}\left(\frac{i}{N+1}\right)\E{\Phi^{-1}\left(\frac{Q_i}{N+1}\right)}\]

\bigskip
\hrule
\bigskip

For the FGM copula, 
\[c_\theta(u,v) = 1 + \theta (1 - 2u)(1 - 2v)\]
and
\[\frac{\partial c_\theta}{\partial \theta}\bigg|_{\theta =0} = (1-2u)(1-2v)\]

\[\frac{1}{N}\sumiN \left(1 - 2 \frac{R_i}{N+1} - 2\frac{S_i}{N+1} + 4 \frac{R_i}{N+1}\frac{S_i}{N+1}\right)\]
but the middle two terms are irrelevant if we have no ties since the sum of the $R_i$ and the sum of the $S_i$ is constant given $N$. Hence this is equivalent to spearman's rho.

\[\frac{\partial}{\partial \theta}\log c_\theta(u,v) \bigg|_{\theta = \theta_0} = \frac{\frac{\partial }{\partial \theta} c_\theta(u,v)}{c_\theta(u,v)}\bigg|_{\theta = \theta_0}\]

We now want to find a statistic such that
\[S=0 \Leftrightarrow X \indep Y \; \text{(no ties)}\]
We have $H_0: C(u,v) = uv$ and we can test this by
\[B_n = \int_0^1 \int_0^1 \! (C_n(u,v) - uv)^2 \du \dv\]
where $C_n$ is the empirical CDF of $\left(\frac{R_i}{N+1},\frac{S_i}{N+1}\right)$. We could also look at 
\[T_n = \sup_{u,v \in [0,1]}|C_n(u,v) - uv|\]
\[n\int_0^1 \int_0^1 \! (C_n(u,v) - uv)^2 \du \dv \leadsto \int_0^1 \int_0^1 \! \{C(u,v)\}^2 \du \dv\]

\bigskip
\hrule
\bigskip
\begin{itemize}
\item Replicates of $B_N$ under $H_0$. 
\[B_{N,1}^*, \ldots, B_{N,M}^*\]
\item Compute $B_N$ from the sample
\[\p{B_N \geq B_{N,obs}} \approx \frac{1}{m}\sum_{i=1}^M \mathds{1}(B_{N,i}^* \geq B_{N,obs})\]
\end{itemize}


\end{document}


